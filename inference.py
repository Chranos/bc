#!/usr/bin/env python3
"""
BLIP2 + LoRA å›¾æ–‡æ£€ç´¢æ¨ç†è„šæœ¬
ç”¨äºå®é™…åº”ç”¨åœºæ™¯çš„å›¾æ–‡æ£€ç´¢
æ”¯æŒ text + scene_category æ‹¼æ¥æ ¼å¼
"""

import os
import json
import torch
import torch.nn.functional as F
from PIL import Image
from torchvision import transforms
from peft import PeftModel
from models.blip2_qformer import Blip2Qformer


# ============================================
# åœºæ™¯ç±»åˆ«å®šä¹‰
# ============================================
SCENE_CATEGORIES = [
    'èŒåœºæ­£è£…', 'èŒåœºä¼‘é—²', 'è¿åŠ¨å¥èº«', 'æˆ·å¤–æ¢é™©', 'å±…å®¶ä¼‘é—²',
    'ç¤¾äº¤èšä¼š', 'æ—…è¡Œåº¦å‡', 'è¿åŠ¨èµ›äº‹', 'å©šç¤¼ç›¸å…³', 'ç‰¹æ®ŠåŠŸèƒ½',
]


class LoRARetrievalPipeline:
    """å¸¦ LoRA çš„å›¾æ–‡æ£€ç´¢ Pipeline"""
    
    def __init__(self, base_checkpoint, lora_checkpoint, device='cuda:4', use_scene_suffix=True):
        """
        Args:
            base_checkpoint: åŸºç¡€ BLIP2 æƒé‡è·¯å¾„
            lora_checkpoint: LoRA é€‚é…å™¨ç›®å½•
            device: è®¾å¤‡
            use_scene_suffix: æ˜¯å¦åœ¨æ–‡æœ¬åæ·»åŠ åœºæ™¯ç±»åˆ«åç¼€
        """
        self.device = device
        self.use_scene_suffix = use_scene_suffix
        self.model = self._load_model(base_checkpoint, lora_checkpoint)
        self.transform = self._get_transform()
        
        if use_scene_suffix:
            print(f"ğŸ’¡ ä½¿ç”¨åœºæ™¯åç¼€æ ¼å¼: text + \"ï¼Œé€‚åˆ\" + scene_category")
        else:
            print(f"ğŸ’¡ ä½¿ç”¨åŸå§‹æ–‡æœ¬æ ¼å¼")
    
    def _load_model(self, base_checkpoint, lora_checkpoint):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹...")
        
        # åŸºç¡€æ¨¡å‹
        model = Blip2Qformer(
            vit_model="clip_L",
            img_size=224,
            freeze_vit=True,
            num_query_token=32,
            embed_dim=256,
            max_txt_len=77,
        )
        
        # åŠ è½½åŸºç¡€æƒé‡
        if os.path.exists(base_checkpoint):
            checkpoint = torch.load(base_checkpoint, map_location='cpu')
            state_dict = checkpoint.get("model", checkpoint)
            model.load_state_dict(state_dict, strict=False)
            print(f"âœ… åŸºç¡€æƒé‡å·²åŠ è½½")
        
        # åŠ è½½ LoRA
        if os.path.exists(lora_checkpoint):
            model.Qformer = PeftModel.from_pretrained(
                model.Qformer,
                lora_checkpoint,
                is_trainable=False
            )
            print(f"âœ… LoRA æƒé‡å·²åŠ è½½")
        
        model.to(self.device)
        model.eval()
        
        return model
    
    def _get_transform(self):
        """å›¾åƒé¢„å¤„ç†"""
        normalize = transforms.Normalize(
            mean=[0.48145466, 0.4578275, 0.40821073],
            std=[0.26862954, 0.26130258, 0.27577711]
        )
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            normalize,
        ])
    
    def format_text_with_scene(self, text, scene_category=None):
        """
        æ ¼å¼åŒ–æ–‡æœ¬ï¼ˆæ·»åŠ åœºæ™¯åç¼€ï¼‰
        
        Args:
            text: åŸå§‹æ–‡æœ¬æè¿°
            scene_category: åœºæ™¯ç±»åˆ«ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            formatted_text: æ ¼å¼åŒ–åçš„æ–‡æœ¬
        """
        if not self.use_scene_suffix:
            return text
        
        if scene_category:
            return f"{text}ï¼Œé€‚åˆ{scene_category}"
        else:
            return text
    
    def preprocess_image(self, image_path):
        """é¢„å¤„ç†å›¾åƒ"""
        image = Image.open(image_path).convert('RGB')
        image = self.transform(image).unsqueeze(0)
        return image
    
    @torch.no_grad()
    def compute_similarity(self, images, texts):
        """
        è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦
        
        Args:
            images: å›¾åƒè·¯å¾„åˆ—è¡¨æˆ–å›¾åƒ tensor
            texts: æ–‡æœ¬åˆ—è¡¨ï¼ˆæˆ–åŒ…å« text/scene_category çš„å­—å…¸åˆ—è¡¨ï¼‰
        
        Returns:
            similarity_matrix: [N_images, N_texts] ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        # å¤„ç†å›¾åƒ
        if isinstance(images, list):
            image_tensors = []
            for img_path in images:
                img = self.preprocess_image(img_path)
                image_tensors.append(img)
            images = torch.cat(image_tensors, dim=0)
        
        images = images.to(self.device)
        
        # å¤„ç†æ–‡æœ¬ï¼ˆæ”¯æŒå­—å…¸æ ¼å¼ï¼‰
        formatted_texts = []
        for text in texts:
            if isinstance(text, dict):
                # å­—å…¸æ ¼å¼: {'text': ..., 'scene_category': ...}
                formatted_text = self.format_text_with_scene(
                    text.get('text', ''),
                    text.get('scene_category', None)
                )
            else:
                # å­—ç¬¦ä¸²æ ¼å¼
                formatted_text = text
            formatted_texts.append(formatted_text)
        
        # æå–ç‰¹å¾
        image_feats, text_feats = self.model({'image': images, 'text': formatted_texts})
        
        # å¤„ç†ç»´åº¦
        if image_feats.dim() == 3:
            image_feats = image_feats.mean(dim=1)
        
        # å½’ä¸€åŒ–
        image_feats = F.normalize(image_feats, dim=-1)
        text_feats = F.normalize(text_feats, dim=-1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = image_feats @ text_feats.t()
        
        return similarity.cpu()
    
    def retrieve_text(self, image_path, text_candidates, top_k=5):
        """
        ç»™å®šå›¾åƒï¼Œæ£€ç´¢æœ€ç›¸å…³çš„æ–‡æœ¬
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            text_candidates: å€™é€‰æ–‡æœ¬åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
            top_k: è¿”å›å‰ k ä¸ªç»“æœ
        
        Returns:
            results: [(text, score), ...]
        """
        similarity = self.compute_similarity([image_path], text_candidates)
        similarity = similarity[0]  # [N_texts]
        
        # æ’åº
        scores, indices = torch.topk(similarity, k=min(top_k, len(text_candidates)))
        
        results = []
        for score, idx in zip(scores, indices):
            text_item = text_candidates[idx.item()]
            
            # è¿”å›åŸå§‹æ–‡æœ¬æˆ–å­—å…¸
            if isinstance(text_item, dict):
                display_text = self.format_text_with_scene(
                    text_item.get('text', ''),
                    text_item.get('scene_category', None)
                )
                results.append((display_text, score.item(), text_item))
            else:
                results.append((text_item, score.item()))
        
        return results
    
    def retrieve_image(self, text, image_paths, top_k=5, scene_category=None):
        """
        ç»™å®šæ–‡æœ¬ï¼Œæ£€ç´¢æœ€ç›¸å…³çš„å›¾åƒ
        
        Args:
            text: æŸ¥è¯¢æ–‡æœ¬ï¼ˆå­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
            image_paths: å€™é€‰å›¾åƒè·¯å¾„åˆ—è¡¨
            top_k: è¿”å›å‰ k ä¸ªç»“æœ
            scene_category: åœºæ™¯ç±»åˆ«ï¼ˆå¯é€‰ï¼Œå¦‚æœ text æ˜¯å­—ç¬¦ä¸²æ—¶ä½¿ç”¨ï¼‰
        
        Returns:
            results: [(image_path, score), ...]
        """
        # å¤„ç†è¾“å…¥æ–‡æœ¬
        if isinstance(text, dict):
            query_texts = [text]
        else:
            query_texts = [{'text': text, 'scene_category': scene_category}]
        
        similarity = self.compute_similarity(image_paths, query_texts)
        similarity = similarity[:, 0]  # [N_images]
        
        # æ’åº
        scores, indices = torch.topk(similarity, k=min(top_k, len(image_paths)))
        
        results = []
        for score, idx in zip(scores, indices):
            results.append((image_paths[idx.item()], score.item()))
        
        return results
    
    def retrieve_from_annotation(self, query_text, annotation_file, image_dir, 
                                  top_k=5, scene_category=None):
        """
        ä»æ ‡æ³¨æ–‡ä»¶ä¸­æ£€ç´¢å›¾åƒ
        
        Args:
            query_text: æŸ¥è¯¢æ–‡æœ¬
            annotation_file: æ ‡æ³¨æ–‡ä»¶è·¯å¾„
            image_dir: å›¾åƒç›®å½•
            top_k: è¿”å›å‰ k ä¸ªç»“æœ
            scene_category: åœºæ™¯ç±»åˆ«ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            results: [(image_path, score, annotation), ...]
        """
        # åŠ è½½æ ‡æ³¨
        with open(annotation_file, 'r', encoding='utf-8') as f:
            annotations = json.load(f)
        
        # æ„å»ºå›¾åƒè·¯å¾„
        image_paths = [os.path.join(image_dir, ann['file_name']) for ann in annotations]
        
        # æ£€ç´¢
        results = self.retrieve_image(query_text, image_paths, top_k, scene_category)
        
        # æ·»åŠ æ ‡æ³¨ä¿¡æ¯
        results_with_ann = []
        for img_path, score in results:
            idx = image_paths.index(img_path)
            results_with_ann.append((img_path, score, annotations[idx]))
        
        return results_with_ann


# ============================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================
def demo_basic():
    """åŸºç¡€æ¨ç†ç¤ºä¾‹"""
    
    print("\n" + "="*60)
    print("ğŸ“ åŸºç¡€æ¨ç†ç¤ºä¾‹")
    print("="*60)
    
    # åˆå§‹åŒ– Pipeline
    pipeline = LoRARetrievalPipeline(
        base_checkpoint='checkpoint_04.pth',
        lora_checkpoint='outputs/fashion_lora_itc_scene/best_model',
        device='cuda:4',
        use_scene_suffix=True  # ä½¿ç”¨åœºæ™¯åç¼€
    )
    
    # ç¤ºä¾‹1: å›¾åƒæ£€ç´¢æ–‡æœ¬ï¼ˆä½¿ç”¨å­—å…¸æ ¼å¼ï¼‰
    print("\n" + "-"*60)
    print("ç¤ºä¾‹1: å›¾åƒæ£€ç´¢æ–‡æœ¬ï¼ˆå¸¦åœºæ™¯ç±»åˆ«ï¼‰")
    print("-"*60)
    
    image_path = "/data/fasion/train/image/010207.jpg"
    
    # å€™é€‰æ–‡æœ¬ï¼ˆå­—å…¸æ ¼å¼ï¼‰
    text_candidates = [
        {'text': 'ä¿®èº«å‰ªè£è¥¿è£…å¥—è£…ï¼Œæ·±è‰²å•†åŠ¡é£æ ¼', 'scene_category': 'èŒåœºæ­£è£…'},
        {'text': 'è½»ä¾¿é€æ°”è¿åŠ¨æœï¼Œé€‚åˆæ—¥å¸¸é”»ç‚¼', 'scene_category': 'è¿åŠ¨å¥èº«'},
        {'text': 'æŸ”è½¯èˆ’é€‚å®¶å±…æœï¼Œä¼‘é—²å®½æ¾', 'scene_category': 'å±…å®¶ä¼‘é—²'},
        {'text': 'å¤šåŠŸèƒ½æˆ·å¤–å†²é”‹è¡£ï¼Œé˜²é£é˜²æ°´', 'scene_category': 'æˆ·å¤–æ¢é™©'},
        {'text': 'ä¼˜é›…æ™šç¤¼æœï¼Œåä¸½è®¾è®¡', 'scene_category': 'ç¤¾äº¤èšä¼š'},
    ]
    
    results = pipeline.retrieve_text(image_path, text_candidates, top_k=3)
    
    print(f"æŸ¥è¯¢å›¾åƒ: {os.path.basename(image_path)}")
    print(f"æœ€åŒ¹é…çš„æ–‡æœ¬:")
    for i, result in enumerate(results, 1):
        if len(result) == 3:  # (text, score, dict)
            text, score, orig = result
            print(f"  {i}. {text}")
            print(f"     ç›¸ä¼¼åº¦: {score:.4f}")
        else:  # (text, score)
            text, score = result
            print(f"  {i}. {text} (ç›¸ä¼¼åº¦: {score:.4f})")
    
    # ç¤ºä¾‹2: æ–‡æœ¬æ£€ç´¢å›¾åƒï¼ˆå¸¦åœºæ™¯ç±»åˆ«ï¼‰
    print("\n" + "-"*60)
    print("ç¤ºä¾‹2: æ–‡æœ¬æ£€ç´¢å›¾åƒï¼ˆå¸¦åœºæ™¯ç±»åˆ«ï¼‰")
    print("-"*60)
    
    query_text = "ä¿®èº«å•†åŠ¡è¥¿è£…ï¼Œé€‚åˆæ­£å¼åœºåˆ"
    scene_category = "èŒåœºæ­£è£…"
    
    image_paths = [
        "/data/fasion/train/image/010207.jpg",
        "/data/fasion/train/image/010208.jpg",
        "/data/fasion/train/image/010209.jpg",
    ]
    
    results = pipeline.retrieve_image(
        query_text, 
        image_paths, 
        top_k=3,
        scene_category=scene_category
    )
    
    print(f"æŸ¥è¯¢æ–‡æœ¬: {query_text}ï¼Œé€‚åˆ{scene_category}")
    print(f"æœ€åŒ¹é…çš„å›¾åƒ:")
    for i, (img_path, score) in enumerate(results, 1):
        print(f"  {i}. {os.path.basename(img_path)} (ç›¸ä¼¼åº¦: {score:.4f})")


def demo_with_annotation():
    """ä½¿ç”¨æ ‡æ³¨æ–‡ä»¶çš„æ¨ç†ç¤ºä¾‹"""
    
    print("\n" + "="*60)
    print("ğŸ“š åŸºäºæ ‡æ³¨æ–‡ä»¶çš„æ£€ç´¢ç¤ºä¾‹")
    print("="*60)
    
    # åˆå§‹åŒ– Pipeline
    pipeline = LoRARetrievalPipeline(
        base_checkpoint='checkpoint_04.pth',
        lora_checkpoint='outputs/fashion_lora_itc_scene/best_model',
        device='cuda:4',
        use_scene_suffix=True
    )
    
    # ä»æ ‡æ³¨æ–‡ä»¶æ£€ç´¢
    query_text = "é€‚åˆå¤å­£ç©¿ç€çš„è½»è–„è¡£ç‰©"
    scene_category = "æ—…è¡Œåº¦å‡"
    
    results = pipeline.retrieve_from_annotation(
        query_text=query_text,
        annotation_file='/workspace/vlm/lab/output/test_split.json',
        image_dir='/data/fasion/train/image',
        top_k=5,
        scene_category=scene_category
    )
    
    print(f"\næŸ¥è¯¢: {query_text}ï¼Œé€‚åˆ{scene_category}")
    print(f"\næ£€ç´¢ç»“æœ:")
    for i, (img_path, score, ann) in enumerate(results, 1):
        print(f"\n{i}. {os.path.basename(img_path)} (ç›¸ä¼¼åº¦: {score:.4f})")
        print(f"   åœºæ™¯: {ann.get('scene_category', 'N/A')}")
        print(f"   æè¿°: {ann.get('text', 'N/A')[:80]}...")


def demo_batch_retrieval():
    """æ‰¹é‡æ£€ç´¢ç¤ºä¾‹"""
    
    print("\n" + "="*60)
    print("ğŸ”„ æ‰¹é‡æ£€ç´¢ç¤ºä¾‹")
    print("="*60)
    
    pipeline = LoRARetrievalPipeline(
        base_checkpoint='checkpoint_04.pth',
        lora_checkpoint='outputs/fashion_lora_itc_scene/best_model',
        device='cuda:4',
        use_scene_suffix=True
    )
    
    # å¤šä¸ªæŸ¥è¯¢
    queries = [
        {'text': 'å•†åŠ¡æ­£è£…è¥¿è£…', 'scene_category': 'èŒåœºæ­£è£…'},
        {'text': 'ä¼‘é—²è¿åŠ¨è£…å¤‡', 'scene_category': 'è¿åŠ¨å¥èº«'},
        {'text': 'èˆ’é€‚å®¶å±…æœ', 'scene_category': 'å±…å®¶ä¼‘é—²'},
    ]
    
    image_paths = [
        "/data/fasion/train/image/010207.jpg",
        "/data/fasion/train/image/010208.jpg",
        "/data/fasion/train/image/010209.jpg",
    ]
    
    # è®¡ç®—æ‰€æœ‰ç›¸ä¼¼åº¦
    similarity = pipeline.compute_similarity(image_paths, queries)
    
    print(f"\nç›¸ä¼¼åº¦çŸ©é˜µ [{len(image_paths)} å›¾åƒ Ã— {len(queries)} æŸ¥è¯¢]:")
    print("-" * 60)
    print(f"{'å›¾åƒ':<20s}", end="")
    for i, q in enumerate(queries):
        print(f"æŸ¥è¯¢{i+1:<3d}", end="  ")
    print()
    print("-" * 60)
    
    for i, img_path in enumerate(image_paths):
        print(f"{os.path.basename(img_path):<20s}", end="")
        for j in range(len(queries)):
            print(f"{similarity[i, j]:.4f}  ", end="")
        print()


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
    demo_basic()
    #demo_with_annotation()
    demo_batch_retrieval()