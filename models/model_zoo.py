"""
æ¨¡å‹åº“ - ç»Ÿä¸€æ¥å£å°è£…ä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹
æ”¯æŒçš„æ¨¡å‹:
- CLIP (OpenAI): ViT-B/32, ViT-L/14
- Chinese-CLIP: ä¸­æ–‡å¤šæ¨¡æ€æ¨¡å‹
- BLIP: Salesforce å›¾æ–‡æ£€ç´¢æ¨¡å‹
- BLIP2 + LoRA: å¾®è°ƒæ¨¡å‹ï¼ˆä½ çš„æ¨¡å‹ï¼‰
- ResNet: çº¯å›¾åƒåˆ†ç±»åŸºçº¿
- ViT: Vision Transformer åˆ†ç±»åŸºçº¿
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from abc import ABC, abstractmethod
from PIL import Image
from typing import List, Tuple, Optional, Union
import warnings

warnings.filterwarnings('ignore')


# ============================================
# åŸºç¡€æ¨¡å‹åŒ…è£…å™¨
# ============================================
class BaseModelWrapper(ABC):
    """åŸºç¡€æ¨¡å‹åŒ…è£…å™¨ - ç»Ÿä¸€æ¥å£"""
    
    def __init__(self, device='cuda'):
        """
        Args:
            device: è¿è¡Œè®¾å¤‡ (cuda/cpu)
        """
        self.device = device
        self.model = None
        self.processor = None
        self.model_name = "BaseModel"
    
    @abstractmethod
    def extract_features(self, images: Union[List[Image.Image], torch.Tensor], 
                        texts: List[str]) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        æå–å›¾åƒå’Œæ–‡æœ¬ç‰¹å¾
        
        Args:
            images: PIL Images åˆ—è¡¨æˆ– tensor [B, C, H, W]
            texts: æ–‡æœ¬åˆ—è¡¨
        
        Returns:
            image_feats: [B, D] å›¾åƒç‰¹å¾
            text_feats: [B, D] æ–‡æœ¬ç‰¹å¾ï¼ˆå¦‚æœæ”¯æŒï¼‰æˆ– None
        """
        pass
    
    @abstractmethod
    def classify_scene(self, images: Union[List[Image.Image], torch.Tensor]) -> Optional[torch.Tensor]:
        """
        åœºæ™¯åˆ†ç±»ï¼ˆå¦‚æœæ”¯æŒï¼‰
        
        Args:
            images: PIL Images åˆ—è¡¨æˆ– tensor
        
        Returns:
            logits: [B, num_classes] åˆ†ç±» logits æˆ– None
        """
        pass
    
    def compute_similarity(self, image_feats: torch.Tensor, 
                          text_feats: torch.Tensor) -> torch.Tensor:
        """
        è®¡ç®—å›¾æ–‡ç›¸ä¼¼åº¦çŸ©é˜µ
        
        Args:
            image_feats: [N, D]
            text_feats: [M, D]
        
        Returns:
            similarity: [N, M]
        """
        image_feats = F.normalize(image_feats, dim=-1)
        text_feats = F.normalize(text_feats, dim=-1)
        return image_feats @ text_feats.t()
    
    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'name': self.model_name,
            'device': str(self.device),
            'supports_text': True,
            'supports_classification': False,
        }


# ============================================
# CLIP æ¨¡å‹å°è£…
# ============================================
class CLIPWrapper(BaseModelWrapper):
    """OpenAI CLIP æ¨¡å‹"""
    
    def __init__(self, model_name='ViT-B/32', device='cuda', checkpoint_path=None, num_classes=10):
        """
        Args:
            model_name: 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'RN50', 'RN101'
            device: è¿è¡Œè®¾å¤‡
            checkpoint_path: å¾®è°ƒåçš„æƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
        """
        super().__init__(device)
        self.model_name = f"CLIP-{model_name}"
        self.clip_model_name = model_name
        
        print(f"ğŸ“¥ åŠ è½½ {self.model_name}...")
        
        try:
            import clip
            self.model, self.preprocess = clip.load(model_name, device=device)
            self.model.eval()
            print(f"âœ… {self.model_name} åŠ è½½æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½ CLIP å¤±è´¥: {e}\nè¯·å®‰è£…: pip install git+https://github.com/openai/CLIP.git")
        
        # è·å–ç‰¹å¾ç»´åº¦
        if 'ViT-B' in model_name:
            self.feature_dim = 512
        elif 'ViT-L' in model_name:
            self.feature_dim = 768
        elif 'RN50' in model_name or 'RN101' in model_name:
            self.feature_dim = 1024
        else:
            self.feature_dim = 512
        
        # åˆ†ç±»å™¨ï¼ˆå¦‚æœæä¾›äº†checkpointåˆ™åŠ è½½å¾®è°ƒæƒé‡ï¼‰
        self.classifier = None
        if checkpoint_path and os.path.exists(checkpoint_path):
            print(f"ğŸ“¥ åŠ è½½å¾®è°ƒæƒé‡: {checkpoint_path}")
            
            # é‡å»ºåˆ†ç±»å™¨ç»“æ„ï¼ˆå¿…é¡»å’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼ï¼‰
            self.classifier = nn.Sequential(
                nn.Linear(self.feature_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, num_classes)
            ).to(device)
            
            # åŠ è½½æƒé‡
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            else:
                state_dict = checkpoint
            
            # æå– classifier æƒé‡
            classifier_state = {}
            clip_state = {}
            
            for key, value in state_dict.items():
                if 'classifier.' in key:
                    new_key = key.replace('classifier.', '')
                    classifier_state[new_key] = value
                elif 'clip_model.' in key:
                    new_key = key.replace('clip_model.', '')
                    clip_state[new_key] = value
            
            # åŠ è½½åˆ†ç±»å™¨
            if classifier_state:
                try:
                    self.classifier.load_state_dict(classifier_state, strict=True)
                    print(f"  âœ… åˆ†ç±»å™¨æƒé‡å·²åŠ è½½ ({len(classifier_state)} ä¸ªå‚æ•°)")
                except Exception as e:
                    print(f"  âš ï¸ åˆ†ç±»å™¨æƒé‡åŠ è½½å¤±è´¥: {e}")
                    print(f"  Keys in checkpoint: {list(classifier_state.keys())[:5]}...")
            else:
                print(f"  âš ï¸ Checkpoint ä¸­æ²¡æœ‰ classifier æƒé‡")
            
            # åŠ è½½ CLIP ç¼–ç å™¨ï¼ˆå¯é€‰ï¼Œé€šå¸¸å†»ç»“ä¸éœ€è¦ï¼‰
            if clip_state:
                try:
                    self.model.load_state_dict(clip_state, strict=False)
                    print(f"  âœ… CLIP ç¼–ç å™¨æƒé‡å·²æ›´æ–°")
                except Exception as e:
                    print(f"  âš ï¸ CLIP æƒé‡åŠ è½½å¤±è´¥ï¼ˆä½¿ç”¨é»˜è®¤ï¼‰: {e}")
            
            self.classifier.eval()
    
    @torch.no_grad()
    def extract_features(self, images, texts):
        """æå– CLIP ç‰¹å¾"""
        # å¤„ç†å›¾åƒ
        if isinstance(images, torch.Tensor):
            # å¦‚æœæ˜¯ tensorï¼Œéœ€è¦è½¬æ¢ä¸º PIL
            from torchvision.transforms import ToPILImage
            to_pil = ToPILImage()
            images = [to_pil(img.cpu()) for img in images]
        
        # é¢„å¤„ç†å›¾åƒ
        image_inputs = torch.stack([self.preprocess(img) for img in images]).to(self.device)
        
        # å¤„ç†æ–‡æœ¬
        import clip
        text_tokens = clip.tokenize(texts, truncate=True).to(self.device)
        
        # æå–ç‰¹å¾
        image_feats = self.model.encode_image(image_inputs)
        text_feats = self.model.encode_text(text_tokens)
        
        # è½¬æ¢ä¸º float32
        image_feats = image_feats.float()
        text_feats = text_feats.float()
        
        return image_feats, text_feats
    
    @torch.no_grad()
    def classify_scene(self, images):
        """åœºæ™¯åˆ†ç±»"""
        if self.classifier is None:
            return None
        
        # æå–å›¾åƒç‰¹å¾
        image_feats, _ = self.extract_features(images, [""])
        
        # åˆ†ç±»
        logits = self.classifier(image_feats)
        return logits
    
    def get_model_info(self):
        info = super().get_model_info()
        info.update({
            'supports_text': True,
            'supports_classification': self.classifier is not None,
            'model_type': 'CLIP',
        })
        return info


# ============================================
# Chinese-CLIP æ¨¡å‹å°è£…
# ============================================
class ChineseCLIPWrapper(BaseModelWrapper):
    """Chinese-CLIP ä¸­æ–‡å¤šæ¨¡æ€æ¨¡å‹"""
    
    def __init__(self, model_name='OFA-Sys/chinese-clip-vit-base-patch16', device='cuda'):
        """
        Args:
            model_name: HuggingFace æ¨¡å‹åç§°
            device: è¿è¡Œè®¾å¤‡
        """
        super().__init__(device)
        self.model_name = "Chinese-CLIP"
        
        print(f"ğŸ“¥ åŠ è½½ {self.model_name}...")
        
        try:
            from transformers import ChineseCLIPProcessor, ChineseCLIPModel
            
            self.processor = ChineseCLIPProcessor.from_pretrained(model_name)
            self.model = ChineseCLIPModel.from_pretrained(model_name).to(device)
            self.model.eval()
            
            print(f"âœ… {self.model_name} åŠ è½½æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½ Chinese-CLIP å¤±è´¥: {e}\nè¯·å®‰è£…: pip install transformers")
    
    @torch.no_grad()
    def extract_features(self, images, texts):
        """æå– Chinese-CLIP ç‰¹å¾"""
        # è½¬æ¢å›¾åƒæ ¼å¼
        if isinstance(images, torch.Tensor):
            from torchvision.transforms import ToPILImage
            to_pil = ToPILImage()
            images = [to_pil(img.cpu()) for img in images]
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            text=texts,
            images=images,
            return_tensors="pt",
            padding=True,
            truncation=True
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # æå–ç‰¹å¾
        outputs = self.model(**inputs)
        image_feats = outputs.image_embeds
        text_feats = outputs.text_embeds
        
        return image_feats, text_feats
    
    def classify_scene(self, images):
        """Chinese-CLIP ä¸ç›´æ¥æ”¯æŒåˆ†ç±»"""
        return None
    
    def get_model_info(self):
        info = super().get_model_info()
        info.update({
            'supports_text': True,
            'supports_classification': False,
            'model_type': 'Chinese-CLIP',
            'language': 'Chinese',
        })
        return info


# ============================================
# BLIP æ¨¡å‹å°è£…
# ============================================
class BLIPWrapper(BaseModelWrapper):
    """BLIP å›¾æ–‡æ£€ç´¢æ¨¡å‹"""
    
    def __init__(self, model_name='Salesforce/blip-itm-base-coco', device='cuda'):
        """
        Args:
            model_name: HuggingFace æ¨¡å‹åç§°
            device: è¿è¡Œè®¾å¤‡
        """
        super().__init__(device)
        self.model_name = "BLIP-Base"
        
        print(f"ğŸ“¥ åŠ è½½ {self.model_name}...")
        
        try:
            from transformers import BlipProcessor, BlipModel
            
            self.processor = BlipProcessor.from_pretrained(model_name)
            self.model = BlipModel.from_pretrained(model_name).to(device)
            self.model.eval()
            
            print(f"âœ… {self.model_name} åŠ è½½æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½ BLIP å¤±è´¥: {e}\nè¯·å®‰è£…: pip install transformers")
    
    @torch.no_grad()
    def extract_features(self, images, texts):
        """æå– BLIP ç‰¹å¾"""
        # è½¬æ¢å›¾åƒæ ¼å¼
        if isinstance(images, torch.Tensor):
            from torchvision.transforms import ToPILImage
            to_pil = ToPILImage()
            images = [to_pil(img.cpu()) for img in images]
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor(
            images=images,
            text=texts,
            return_tensors="pt",
            padding=True,
            truncation=True
        )
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # æå–ç‰¹å¾
        outputs = self.model(**inputs)
        
        # BLIP çš„ç‰¹å¾åœ¨ vision_model_output å’Œ text_model_output ä¸­
        image_feats = outputs.image_embeds
        text_feats = outputs.text_embeds
        
        return image_feats, text_feats
    
    def classify_scene(self, images):
        """BLIP ä¸ç›´æ¥æ”¯æŒåˆ†ç±»"""
        return None
    
    def get_model_info(self):
        info = super().get_model_info()
        info.update({
            'supports_text': True,
            'supports_classification': False,
            'model_type': 'BLIP',
        })
        return info


# ============================================
# BLIP2 + LoRA æ¨¡å‹å°è£…
# ============================================
class BLIP2LoRAWrapper(BaseModelWrapper):
    """BLIP2 + LoRA å¾®è°ƒæ¨¡å‹ï¼ˆä½ çš„æ¨¡å‹ï¼‰"""
    
    def __init__(self, base_checkpoint: str, lora_checkpoint: str, 
                 scene_head_path: Optional[str] = None, device='cuda'):
        """
        Args:
            base_checkpoint: åŸºç¡€ BLIP2 æƒé‡è·¯å¾„
            lora_checkpoint: LoRA é€‚é…å™¨ç›®å½•
            scene_head_path: åœºæ™¯åˆ†ç±»å¤´æƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            device: è¿è¡Œè®¾å¤‡
        """
        super().__init__(device)
        self.model_name = "BLIP2-LoRA (Ours)"
        
        print(f"ğŸ“¥ åŠ è½½ {self.model_name}...")
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(base_checkpoint):
            raise FileNotFoundError(f"åŸºç¡€æƒé‡ä¸å­˜åœ¨: {base_checkpoint}")
        if not os.path.exists(lora_checkpoint):
            raise FileNotFoundError(f"LoRA æƒé‡ä¸å­˜åœ¨: {lora_checkpoint}")
        
        try:
            # å¯¼å…¥æœ¬åœ°æ¨¡å‹
            import sys
            sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
            from models.blip2_qformer import Blip2Qformer
            from peft import PeftModel
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            self.model = Blip2Qformer(
                vit_model="clip_L",
                img_size=224,
                freeze_vit=True,
                num_query_token=32,
                embed_dim=256,
                max_txt_len=77,
            )
            
            # åŠ è½½åŸºç¡€æƒé‡
            checkpoint = torch.load(base_checkpoint, map_location='cpu')
            state_dict = checkpoint.get("model", checkpoint)
            missing, unexpected = self.model.load_state_dict(state_dict, strict=False)
            
            if missing:
                print(f"  âš ï¸ ç¼ºå¤±çš„é”®: {len(missing)} ä¸ª")
            if unexpected:
                print(f"  âš ï¸ æœªé¢„æœŸçš„é”®: {len(unexpected)} ä¸ª")
            
            # åŠ è½½ LoRA
            self.model.Qformer = PeftModel.from_pretrained(
                self.model.Qformer,
                lora_checkpoint,
                is_trainable=False
            )
            
            self.model.to(device)
            self.model.eval()
            
            print(f"âœ… BLIP2 + LoRA åŠ è½½æˆåŠŸ")
            
            # åŠ è½½åœºæ™¯åˆ†ç±»å¤´
            self.scene_head = None
            if scene_head_path and os.path.exists(scene_head_path):
                # å®šä¹‰åœºæ™¯åˆ†ç±»å¤´ç»“æ„ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
                class SceneClassificationHead(nn.Module):
                    def __init__(self, input_dim=256, num_classes=10, dropout=0.1):
                        super().__init__()
                        self.classifier = nn.Sequential(
                            nn.Linear(input_dim, input_dim),
                            nn.ReLU(),
                            nn.Dropout(dropout),
                            nn.Linear(input_dim, num_classes)
                        )
                    
                    def forward(self, x):
                        return self.classifier(x)
                
                self.scene_head = SceneClassificationHead(
                    input_dim=256,
                    num_classes=10,
                    dropout=0.1
                ).to(device)
                
                state_dict = torch.load(scene_head_path, map_location='cpu')
                self.scene_head.load_state_dict(state_dict)
                self.scene_head.eval()
                
                print(f"âœ… åœºæ™¯åˆ†ç±»å¤´åŠ è½½æˆåŠŸ")
            
            # è®¾ç½®å›¾åƒé¢„å¤„ç†
            from torchvision import transforms
            normalize = transforms.Normalize(
                mean=[0.48145466, 0.4578275, 0.40821073],
                std=[0.26862954, 0.26130258, 0.27577711]
            )
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                normalize,
            ])
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"âŒ åŠ è½½ BLIP2-LoRA å¤±è´¥: {e}")
    
    @torch.no_grad()
    def extract_features(self, images, texts):
        """æå– BLIP2 ç‰¹å¾"""
        # å¤„ç†å›¾åƒ
        if isinstance(images, list):
            # PIL Images
            image_tensors = torch.stack([self.transform(img) for img in images])
        else:
            # å·²ç»æ˜¯ tensor
            image_tensors = images
        
        image_tensors = image_tensors.to(self.device)
        
        # æå–ç‰¹å¾
        image_feats, text_feats = self.model({'image': image_tensors, 'text': texts})
        
        # å¤„ç†ç»´åº¦
        if image_feats.dim() == 3:
            # [B, num_queries, D] -> [B, D]
            image_feats = image_feats.mean(dim=1)
        
        return image_feats, text_feats
    
    @torch.no_grad()
    def classify_scene(self, images):
        """åœºæ™¯åˆ†ç±»"""
        if self.scene_head is None:
            print("âš ï¸ åœºæ™¯åˆ†ç±»å¤´æœªåŠ è½½")
            return None
        
        # æå–å›¾åƒç‰¹å¾
        image_feats, _ = self.extract_features(images, [""] * len(images))
        
        # åˆ†ç±»
        logits = self.scene_head(image_feats)
        return logits
    
    def get_model_info(self):
        info = super().get_model_info()
        info.update({
            'supports_text': True,
            'supports_classification': self.scene_head is not None,
            'model_type': 'BLIP2-LoRA',
            'has_lora': True,
        })
        return info


# ============================================
# ResNet åˆ†ç±»åŸºçº¿
# ============================================
class ResNetClassifier(BaseModelWrapper):
    """ResNet åœºæ™¯åˆ†ç±»åŸºçº¿"""
    
    def __init__(self, num_classes=10, device='cuda', pretrained=True, 
                 checkpoint_path: Optional[str] = None):
        """
        Args:
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            device: è¿è¡Œè®¾å¤‡
            pretrained: æ˜¯å¦ä½¿ç”¨ ImageNet é¢„è®­ç»ƒ
            checkpoint_path: å¾®è°ƒåçš„æƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        super().__init__(device)
        self.model_name = "ResNet-50"
        
        print(f"ğŸ“¥ åŠ è½½ {self.model_name}...")
        
        try:
            from torchvision import models, transforms
            
            # åŠ è½½æ¨¡å‹
            self.model = models.resnet50(pretrained=pretrained)
            
            # è·å–ç‰¹å¾ç»´åº¦
            feature_dim = self.model.fc.in_features
            
            # æ›¿æ¢åˆ†ç±»å±‚ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼ï¼‰
            self.model.fc = nn.Sequential(
                nn.Linear(feature_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, num_classes)
            )
            
            # åŠ è½½å¾®è°ƒæƒé‡
            if checkpoint_path and os.path.exists(checkpoint_path):
                print(f"ğŸ“¥ åŠ è½½å¾®è°ƒæƒé‡: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                if 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                else:
                    state_dict = checkpoint
                
                # å¤„ç†å¯èƒ½çš„ key å‰ç¼€
                model_state = {}
                for key, value in state_dict.items():
                    new_key = key.replace('backbone.', '') if 'backbone.' in key else key
                    model_state[new_key] = value
                
                try:
                    self.model.load_state_dict(model_state, strict=True)
                    print(f"  âœ… å¾®è°ƒæƒé‡å·²åŠ è½½")
                except Exception as e:
                    print(f"  âš ï¸ æƒé‡åŠ è½½å¤±è´¥ï¼Œå°è¯•éƒ¨åˆ†åŠ è½½: {e}")
                    self.model.load_state_dict(model_state, strict=False)
            
            self.model.to(device)
            self.model.eval()
            
            # å›¾åƒé¢„å¤„ç†
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                )
            ])
            
            print(f"âœ… {self.model_name} åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½ ResNet å¤±è´¥: {e}")
    
    @torch.no_grad()
    def extract_features(self, images, texts):
        """æå– ResNet ç‰¹å¾ï¼ˆä¸æ”¯æŒæ–‡æœ¬ï¼‰"""
        # å¤„ç†å›¾åƒ
        if isinstance(images, list):
            image_tensors = torch.stack([self.transform(img) for img in images])
        else:
            image_tensors = images
        
        image_tensors = image_tensors.to(self.device)
        
        # æå–ç‰¹å¾ï¼ˆå»æ‰æœ€åçš„åˆ†ç±»å±‚ï¼‰
        x = self.model.conv1(image_tensors)
        x = self.model.bn1(x)
        x = self.model.relu(x)
        x = self.model.maxpool(x)
        
        x = self.model.layer1(x)
        x = self.model.layer2(x)
        x = self.model.layer3(x)
        x = self.model.layer4(x)
        
        x = self.model.avgpool(x)
        features = torch.flatten(x, 1)
        
        return features, None  # ä¸æ”¯æŒæ–‡æœ¬ç‰¹å¾
    
    @torch.no_grad()
    def classify_scene(self, images):
        """åœºæ™¯åˆ†ç±»"""
        # å¤„ç†å›¾åƒ
        if isinstance(images, list):
            image_tensors = torch.stack([self.transform(img) for img in images])
        else:
            image_tensors = images
        
        image_tensors = image_tensors.to(self.device)
        
        # åˆ†ç±»
        logits = self.model(image_tensors)
        return logits
    
    def get_model_info(self):
        info = super().get_model_info()
        info.update({
            'supports_text': False,
            'supports_classification': True,
            'model_type': 'ResNet',
        })
        return info


# ============================================
# ViT åˆ†ç±»åŸºçº¿
# ============================================
class ViTClassifier(BaseModelWrapper):
    """Vision Transformer åœºæ™¯åˆ†ç±»åŸºçº¿"""
    
    def __init__(self, model_name='vit_base_patch16_224', num_classes=10, 
                 device='cuda', pretrained=True, checkpoint_path: Optional[str] = None):
        """
        Args:
            model_name: ViT æ¨¡å‹åç§°
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            device: è¿è¡Œè®¾å¤‡
            pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒ
            checkpoint_path: å¾®è°ƒåçš„æƒé‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        super().__init__(device)
        self.model_name = f"ViT-{model_name}"
        
        print(f"ğŸ“¥ åŠ è½½ {self.model_name}...")
        
        try:
            import timm
            from torchvision import transforms
            
            # åŠ è½½ backboneï¼ˆä¸å¸¦åˆ†ç±»å¤´ï¼‰
            self.backbone = timm.create_model(
                model_name,
                pretrained=pretrained,
                num_classes=0  # ä¸è¦åˆ†ç±»å¤´
            )
            
            # è·å–ç‰¹å¾ç»´åº¦
            feature_dim = self.backbone.num_features
            
            # åˆ›å»ºåˆ†ç±»å¤´ï¼ˆå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼ï¼‰
            self.classifier = nn.Sequential(
                nn.Linear(feature_dim, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, num_classes)
            )
            
            # åŠ è½½å¾®è°ƒæƒé‡
            if checkpoint_path and os.path.exists(checkpoint_path):
                print(f"ğŸ“¥ åŠ è½½å¾®è°ƒæƒé‡: {checkpoint_path}")
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                
                if 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                else:
                    state_dict = checkpoint
                
                # åˆ†ç¦» backbone å’Œ classifier
                backbone_state = {}
                classifier_state = {}
                
                for key, value in state_dict.items():
                    if 'backbone.' in key:
                        new_key = key.replace('backbone.', '')
                        backbone_state[new_key] = value
                    elif 'classifier.' in key:
                        new_key = key.replace('classifier.', '')
                        classifier_state[new_key] = value
                
                # åŠ è½½æƒé‡
                if backbone_state:
                    self.backbone.load_state_dict(backbone_state, strict=False)
                    print(f"  âœ… Backbone æƒé‡å·²åŠ è½½")
                
                if classifier_state:
                    self.classifier.load_state_dict(classifier_state, strict=True)
                    print(f"  âœ… åˆ†ç±»å™¨æƒé‡å·²åŠ è½½")
            
            self.backbone.to(device)
            self.classifier.to(device)
            self.backbone.eval()
            self.classifier.eval()
            
            # å›¾åƒé¢„å¤„ç†
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                )
            ])
            
            print(f"âœ… {self.model_name} åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½ ViT å¤±è´¥: {e}\nè¯·å®‰è£…: pip install timm")
    
    @torch.no_grad()
    def extract_features(self, images, texts):
        """æå– ViT ç‰¹å¾ï¼ˆä¸æ”¯æŒæ–‡æœ¬ï¼‰"""
        # å¤„ç†å›¾åƒ
        if isinstance(images, list):
            image_tensors = torch.stack([self.transform(img) for img in images])
        else:
            image_tensors = images
        
        image_tensors = image_tensors.to(self.device)
        
        # æå–ç‰¹å¾
        features = self.backbone(image_tensors)
        
        # å¤„ç†ç»´åº¦
        if features.dim() == 3:  # [B, N, D]
            features = features.mean(dim=1)  # [B, D]
        
        return features, None
    
    @torch.no_grad()
    def classify_scene(self, images):
        """åœºæ™¯åˆ†ç±»"""
        # å¤„ç†å›¾åƒ
        if isinstance(images, list):
            image_tensors = torch.stack([self.transform(img) for img in images])
        else:
            image_tensors = images
        
        image_tensors = image_tensors.to(self.device)
        
        # æå–ç‰¹å¾ + åˆ†ç±»
        features = self.backbone(image_tensors)
        logits = self.classifier(features)
        return logits
    
    def get_model_info(self):
        info = super().get_model_info()
        info.update({
            'supports_text': False,
            'supports_classification': True,
            'model_type': 'ViT',
        })
        return info


# ============================================
# æ¨¡å‹å·¥å‚
# ============================================
def create_model(model_name: str, device='cuda', **kwargs) -> BaseModelWrapper:
    """
    åˆ›å»ºæ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§°
            - 'clip-vit-b32': CLIP ViT-B/32
            - 'clip-vit-b16': CLIP ViT-B/16
            - 'clip-vit-l14': CLIP ViT-L/14
            - 'clip-rn50': CLIP ResNet-50
            - 'chinese-clip': Chinese-CLIP
            - 'blip-base': BLIP Base
            - 'blip2-lora': BLIP2 + LoRA (ä½ çš„æ¨¡å‹)
            - 'resnet50': ResNet-50
            - 'vit-base': ViT-Base
        device: è®¾å¤‡ (cuda/cpu)
        **kwargs: é¢å¤–å‚æ•°
            - checkpoint_path: å¾®è°ƒæƒé‡è·¯å¾„
            - num_classes: åˆ†ç±»ç±»åˆ«æ•° (default: 10)
            - base_checkpoint: BLIP2 åŸºç¡€æƒé‡ (for blip2-lora)
            - lora_checkpoint: LoRA æƒé‡ (for blip2-lora)
            - scene_head_path: åˆ†ç±»å¤´æƒé‡ (for blip2-lora)
    
    Returns:
        model: æ¨¡å‹åŒ…è£…å™¨å®ä¾‹
    """
    
    model_registry = {
        # CLIP ç³»åˆ—
        'clip-vit-b32': lambda: CLIPWrapper('ViT-B/32', device, **kwargs),
        'clip-vit-b16': lambda: CLIPWrapper('ViT-B/16', device, **kwargs),
        'clip-vit-l14': lambda: CLIPWrapper('ViT-L/14', device, **kwargs),
        'clip-rn50': lambda: CLIPWrapper('RN50', device, **kwargs),
        'clip-rn101': lambda: CLIPWrapper('RN101', device, **kwargs),
        
        # Chinese-CLIP
        'chinese-clip': lambda: ChineseCLIPWrapper(device=device),
        'chinese-clip-large': lambda: ChineseCLIPWrapper(
            model_name='OFA-Sys/chinese-clip-vit-large-patch14',
            device=device
        ),
        
        # BLIP
        'blip-base': lambda: BLIPWrapper(device=device),
        'blip-large': lambda: BLIPWrapper(
            model_name='Salesforce/blip-itm-large-coco',
            device=device
        ),
        
        # BLIP2 + LoRA
        'blip2-lora': lambda: BLIP2LoRAWrapper(
            base_checkpoint=kwargs.get('base_checkpoint'),
            lora_checkpoint=kwargs.get('lora_checkpoint'),
            scene_head_path=kwargs.get('scene_head_path'),
            device=device
        ),
        
        # åˆ†ç±»åŸºçº¿
        'resnet50': lambda: ResNetClassifier(
            num_classes=kwargs.get('num_classes', 10),
            device=device,
            pretrained=kwargs.get('pretrained', True),
            checkpoint_path=kwargs.get('checkpoint_path'),
        ),
        'vit-base': lambda: ViTClassifier(
            model_name='vit_base_patch16_224',
            num_classes=kwargs.get('num_classes', 10),
            device=device,
            pretrained=kwargs.get('pretrained', True),
            checkpoint_path=kwargs.get('checkpoint_path'),
        ),
    }
    
    if model_name not in model_registry:
        available = ', '.join(model_registry.keys())
        raise ValueError(
            f"âŒ æœªçŸ¥çš„æ¨¡å‹: {model_name}\n"
            f"å¯ç”¨æ¨¡å‹: {available}"
        )
    
    try:
        model = model_registry[model_name]()
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ: {model.model_name}")
        return model
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise RuntimeError(f"âŒ åˆ›å»ºæ¨¡å‹ {model_name} å¤±è´¥: {e}")


# ============================================
# è¾…åŠ©å‡½æ•°
# ============================================
def list_available_models() -> List[str]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
    return [
        'clip-vit-b32', 'clip-vit-b16', 'clip-vit-l14', 'clip-rn50',
        'chinese-clip', 'blip-base', 'blip2-lora', 
        'resnet50', 'vit-base'
    ]


def print_model_info(model: BaseModelWrapper):
    """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
    info = model.get_model_info()
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯: {info['name']}")
    print(f"{'='*60}")
    print(f"  è®¾å¤‡: {info['device']}")
    print(f"  æ”¯æŒæ–‡æœ¬: {'âœ…' if info['supports_text'] else 'âŒ'}")
    print(f"  æ”¯æŒåˆ†ç±»: {'âœ…' if info['supports_classification'] else 'âŒ'}")
    print(f"  æ¨¡å‹ç±»å‹: {info.get('model_type', 'Unknown')}")
    if 'language' in info:
        print(f"  è¯­è¨€: {info['language']}")
    if 'has_lora' in info:
        print(f"  ä½¿ç”¨ LoRA: âœ…")
    print(f"{'='*60}\n")


# ============================================
# æµ‹è¯•ä»£ç 
# ============================================
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹åº“\n")
    
    # æµ‹è¯•åˆ›å»º CLIP æ¨¡å‹
    try:
        print("æµ‹è¯• 1: CLIP-ViT-B/32")
        model = create_model('clip-vit-b32', device='cpu')
        print_model_info(model)
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}\n")
    
    # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
    print("\nğŸ“š æ‰€æœ‰å¯ç”¨æ¨¡å‹:")
    for model_name in list_available_models():
        print(f"  - {model_name}")
    
    print("\nâœ… æ¨¡å‹åº“æµ‹è¯•å®Œæˆ")