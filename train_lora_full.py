#!/usr/bin/env python3
# filepath: /workspace/vlm/lab/BLIP2-Chinese/train_lora_fashion_scene.py

"""
BLIP2 Q-Former LoRA å¾®è°ƒè„šæœ¬ - æ—¶å°šåœºæ™¯åˆ†ç±»ç‰ˆæœ¬
ä½¿ç”¨ ITC + åœºæ™¯åˆ†ç±»è”åˆæŸå¤±
- ITC (Image-Text Contrastive): å›¾æ–‡å¯¹æ¯”å­¦ä¹ 
- Scene Classification: 10ç±»åœºæ™¯åˆ†ç±»

æ•°æ®é›†æ ¼å¼é€‚é…ï¼š
- æ•°æ®è·¯å¾„: /workspace/vlm/lab/output/scene_annotations.json
- å›¾ç‰‡è·¯å¾„: /data/fasion/train/image/{file_name}
"""

import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.tensorboard import SummaryWriter
from torch.cuda.amp import autocast, GradScaler

from PIL import Image
from torchvision import transforms
from tqdm import tqdm
import time
import random

from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from models.blip2_qformer import Blip2Qformer


# ============================================
# åœºæ™¯ç±»åˆ«å®šä¹‰
# ============================================
SCENE_CATEGORIES = [
    'èŒåœºæ­£è£…',      # 0
    'èŒåœºä¼‘é—²',      # 1
    'è¿åŠ¨å¥èº«',      # 2
    'æˆ·å¤–æ¢é™©',      # 3
    'å±…å®¶ä¼‘é—²',      # 4
    'ç¤¾äº¤èšä¼š',      # 5
    'æ—…è¡Œåº¦å‡',      # 6
    'è¿åŠ¨èµ›äº‹',      # 7
    'å©šç¤¼ç›¸å…³',      # 8
    'ç‰¹æ®ŠåŠŸèƒ½',      # 9
]

SCENE_TO_ID = {name: idx for idx, name in enumerate(SCENE_CATEGORIES)}
ID_TO_SCENE = {idx: name for idx, name in enumerate(SCENE_CATEGORIES)}
NUM_SCENE_CLASSES = len(SCENE_CATEGORIES)

print(f"ğŸ“‹ åœºæ™¯ç±»åˆ« ({NUM_SCENE_CLASSES} ç±»):")
for idx, name in enumerate(SCENE_CATEGORIES):
    print(f"  {idx}: {name}")


# ============================================
# æ•°æ®é›†å®šä¹‰ï¼ˆé€‚é…æ—¶å°šæ•°æ®é›†ï¼‰
# ============================================
class FashionSceneDataset(Dataset):
    """
    æ—¶å°šåœºæ™¯åˆ†ç±»æ•°æ®é›†
    
    æ•°æ®æ ¼å¼:
    {
        "file_name": "010207.jpg",
        "scene_category": "å±…å®¶ä¼‘é—²",
        "text": "æè¿°æ–‡æœ¬",
        "key_features": "å…³é”®ç‰¹å¾",
        "suitable_occasion": "é€‚ç”¨åœºåˆ"
    }
    """
    def __init__(self, annotation_file, image_dir, transform=None, 
                 use_key_features=False, max_length=77):
        """
        Args:
            annotation_file: JSON æ ‡æ³¨æ–‡ä»¶è·¯å¾„
            image_dir: å›¾ç‰‡ç›®å½•è·¯å¾„
            transform: å›¾åƒå˜æ¢
            use_key_features: æ˜¯å¦ä½¿ç”¨ key_features ä½œä¸ºæ–‡æœ¬ï¼ˆå¦åˆ™ä½¿ç”¨ text + "ï¼Œé€‚åˆ" + scene_categoryï¼‰
            max_length: æœ€å¤§æ–‡æœ¬é•¿åº¦
        """
        print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {annotation_file}")
        
        with open(annotation_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        self.image_dir = image_dir
        self.transform = transform or self._default_transform()
        self.use_key_features = use_key_features
        self.max_length = max_length
        
        # éªŒè¯å’Œè½¬æ¢åœºæ™¯æ ‡ç­¾
        valid_data = []
        invalid_count = 0
        
        for idx, item in enumerate(self.data):
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            required_fields = ['file_name', 'scene_category']
            if not all(field in item for field in required_fields):
                print(f"âš ï¸ æ•°æ®é¡¹ {idx} ç¼ºå°‘å¿…éœ€å­—æ®µï¼Œè·³è¿‡")
                invalid_count += 1
                continue
            
            # éªŒè¯åœºæ™¯ç±»åˆ«
            scene = item['scene_category']
            if scene not in SCENE_TO_ID:
                print(f"âš ï¸ æœªçŸ¥åœºæ™¯ç±»åˆ« '{scene}' (æ–‡ä»¶: {item['file_name']})ï¼Œè·³è¿‡")
                invalid_count += 1
                continue
            
            # éªŒè¯å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            image_path = os.path.join(self.image_dir, item['file_name'])
            if not os.path.exists(image_path):
                print(f"âš ï¸ å›¾ç‰‡ä¸å­˜åœ¨: {image_path}ï¼Œè·³è¿‡")
                invalid_count += 1
                continue
            
            # è½¬æ¢åœºæ™¯ä¸ºID
            item['scene_id'] = SCENE_TO_ID[scene]
            valid_data.append(item)
        
        self.data = valid_data
        
        if invalid_count > 0:
            print(f"âš ï¸ è·³è¿‡ {invalid_count} ä¸ªæ— æ•ˆæ•°æ®é¡¹")
        
        print(f"ğŸ“Š æœ‰æ•ˆæ•°æ®: {len(self.data)} ä¸ªæ ·æœ¬")
        
        # ç»Ÿè®¡å„åœºæ™¯åˆ†å¸ƒ
        scene_counts = {}
        for item in self.data:
            scene_id = item['scene_id']
            scene_counts[scene_id] = scene_counts.get(scene_id, 0) + 1
        
        print(f"\nğŸ“ˆ åœºæ™¯åˆ†å¸ƒ:")
        for scene_id in sorted(scene_counts.keys()):
            count = scene_counts[scene_id]
            ratio = count / len(self.data) * 100
            scene_name = ID_TO_SCENE[scene_id]
            print(f"  {scene_name:8s}: {count:4d} ({ratio:5.1f}%)")
    
    def _default_transform(self):
        """é»˜è®¤çš„å›¾åƒå˜æ¢"""
        normalize = transforms.Normalize(
            mean=[0.48145466, 0.4578275, 0.40821073],
            std=[0.26862954, 0.26130258, 0.27577711]
        )
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            normalize,
        ])
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # åŠ è½½å›¾åƒ
        image_path = os.path.join(self.image_dir, item['file_name'])
        try:
            image = Image.open(image_path).convert('RGB')
            image = self.transform(image)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å›¾åƒå¤±è´¥: {image_path}, é”™è¯¯: {e}")
            image = torch.zeros(3, 224, 224)
        
        # ===== ä¿®æ”¹ï¼šæ‹¼æ¥ text å’Œ scene_category =====
        if self.use_key_features and 'key_features' in item:
            # ä½¿ç”¨ key_features
            text = item['key_features']
        else:
            # æ‹¼æ¥ text + "ï¼Œé€‚åˆ" + scene_category
            text_content = item.get('text', '')
            scene_category = item.get('scene_category', '')
            
            # æ ¼å¼ï¼štext + "ï¼Œé€‚åˆ" + scene_category
            if text_content and scene_category:
                text = f"{text_content}ï¼Œé€‚åˆ{scene_category}"
            elif text_content:
                text = text_content
            else:
                text = scene_category
        
        return {
            'image': image,
            'text': text,
            'scene_label': item['scene_id'],
            'file_name': item['file_name'],  # ç”¨äºè°ƒè¯•
        }

def split_dataset(annotation_file, train_ratio=0.8, val_ratio=0.1, seed=42):
    """
    åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
    
    Args:
        annotation_file: åŸå§‹æ ‡æ³¨æ–‡ä»¶
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        seed: éšæœºç§å­
    
    Returns:
        train_file, val_file, test_file: åˆ†å‰²åçš„æ–‡ä»¶è·¯å¾„
    """
    print(f"\nğŸ“Š åˆ’åˆ†æ•°æ®é›† (train={train_ratio:.0%}, val={val_ratio:.0%}, test={1-train_ratio-val_ratio:.0%})...")
    
    with open(annotation_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æŒ‰åœºæ™¯åˆ†å±‚åˆ’åˆ†
    scene_data = {}
    for item in data:
        scene = item.get('scene_category', 'unknown')
        if scene not in scene_data:
            scene_data[scene] = []
        scene_data[scene].append(item)
    
    random.seed(seed)
    
    train_data = []
    val_data = []
    test_data = []
    
    for scene, items in scene_data.items():
        random.shuffle(items)
        n = len(items)
        n_train = int(n * train_ratio)
        n_val = int(n * val_ratio)
        
        train_data.extend(items[:n_train])
        val_data.extend(items[n_train:n_train+n_val])
        test_data.extend(items[n_train+n_val:])
    
    # ä¿å­˜åˆ†å‰²åçš„æ•°æ®
    output_dir = os.path.dirname(annotation_file)
    
    train_file = os.path.join(output_dir, 'train_split.json')
    val_file = os.path.join(output_dir, 'val_split.json')
    test_file = os.path.join(output_dir, 'test_split.json')
    
    with open(train_file, 'w', encoding='utf-8') as f:
        json.dump(train_data, f, ensure_ascii=False, indent=2)
    
    with open(val_file, 'w', encoding='utf-8') as f:
        json.dump(val_data, f, ensure_ascii=False, indent=2)
    
    with open(test_file, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬ -> {train_file}")
    print(f"  éªŒè¯é›†: {len(val_data)} æ ·æœ¬ -> {val_file}")
    print(f"  æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬ -> {test_file}")
    
    return train_file, val_file, test_file


def get_train_transform():
    """è®­ç»ƒæ—¶çš„æ•°æ®å¢å¼º"""
    normalize = transforms.Normalize(
        mean=[0.48145466, 0.4578275, 0.40821073],
        std=[0.26862954, 0.26130258, 0.27577711]
    )
    return transforms.Compose([
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),
        transforms.RandomGrayscale(p=0.1),
        transforms.ToTensor(),
        normalize,
    ])


def get_val_transform():
    """éªŒè¯æ—¶ä¸å¢å¼º"""
    normalize = transforms.Normalize(
        mean=[0.48145466, 0.4578275, 0.40821073],
        std=[0.26862954, 0.26130258, 0.27577711]
    )
    return transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        normalize,
    ])


# ============================================
# æŸå¤±å‡½æ•°1ï¼šITC (Image-Text Contrastive)
# ============================================
def compute_itc_loss(image_feats, text_feats, temp, device):
    """
    å›¾æ–‡å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆInfoNCEï¼‰
    
    Args:
        image_feats: [B, num_queries, embed_dim] æˆ– [B, embed_dim]
        text_feats: [B, embed_dim]
        temp: æ¸©åº¦å‚æ•°
        device: è®¾å¤‡
    
    Returns:
        loss: ITC æŸå¤±
        acc_i2t: å›¾->æ–‡æ£€ç´¢å‡†ç¡®ç‡
        acc_t2i: æ–‡->å›¾æ£€ç´¢å‡†ç¡®ç‡
    """
    # 1. å¤„ç† image_feats ç»´åº¦
    if image_feats.dim() == 3:
        image_feats = image_feats.mean(dim=1)
    
    # 2. L2 å½’ä¸€åŒ–
    image_feats = F.normalize(image_feats, dim=-1)
    text_feats = F.normalize(text_feats, dim=-1)
    
    # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    sim_i2t = image_feats @ text_feats.t() / temp
    sim_t2i = sim_i2t.t()
    
    # 4. æ ‡ç­¾
    labels = torch.arange(image_feats.size(0), device=device)
    
    # 5. åŒå‘äº¤å‰ç†µæŸå¤±
    loss_i2t = F.cross_entropy(sim_i2t, labels)
    loss_t2i = F.cross_entropy(sim_t2i, labels)
    loss = (loss_i2t + loss_t2i) / 2
    
    # 6. è®¡ç®—å‡†ç¡®ç‡
    with torch.no_grad():
        acc_i2t = (sim_i2t.argmax(dim=1) == labels).float().mean()
        acc_t2i = (sim_t2i.argmax(dim=1) == labels).float().mean()
    
    return loss, acc_i2t.item(), acc_t2i.item()


# ============================================
# æŸå¤±å‡½æ•°2ï¼šScene Classification
# ============================================
class SceneClassificationHead(nn.Module):
    """åœºæ™¯åˆ†ç±»å¤´"""
    def __init__(self, input_dim=256, num_classes=10, dropout=0.1):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, input_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(input_dim, num_classes)
        )
    
    def forward(self, x):
        """
        Args:
            x: [B, embed_dim] å›¾åƒç‰¹å¾
        Returns:
            logits: [B, num_classes]
        """
        return self.classifier(x)


def compute_scene_loss(scene_head, image_feats, scene_labels, device):
    """
    åœºæ™¯åˆ†ç±»æŸå¤±
    
    Args:
        scene_head: åœºæ™¯åˆ†ç±»å¤´
        image_feats: [B, num_queries, embed_dim] æˆ– [B, embed_dim]
        scene_labels: [B] åœºæ™¯æ ‡ç­¾
        device: è®¾å¤‡
    
    Returns:
        loss: åˆ†ç±»æŸå¤±
        acc: åˆ†ç±»å‡†ç¡®ç‡
    """
    # 1. å¤„ç† image_feats ç»´åº¦
    if image_feats.dim() == 3:
        image_feats = image_feats.mean(dim=1)  # [B, embed_dim]
    
    # 2. åˆ†ç±»é¢„æµ‹
    logits = scene_head(image_feats)  # [B, num_classes]
    
    # 3. è®¡ç®—æŸå¤±
    loss = F.cross_entropy(logits, scene_labels)
    
    # 4. è®¡ç®—å‡†ç¡®ç‡
    with torch.no_grad():
        pred = logits.argmax(dim=1)
        acc = (pred == scene_labels).float().mean()
    
    return loss, acc.item()


# ============================================
# æ¨¡å‹åŠ è½½
# ============================================
def load_base_model(checkpoint_path):
    """åŠ è½½ BLIP2 åŸºç¡€æ¨¡å‹"""
    print(f"\nğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹...")
    
    model = Blip2Qformer(
        vit_model="clip_L",
        img_size=224,
        freeze_vit=True,
        num_query_token=32,
        embed_dim=256,
        max_txt_len=77,  # å¢åŠ æ–‡æœ¬é•¿åº¦ä»¥é€‚åº”è¯¦ç»†æè¿°
    )
    
    if os.path.exists(checkpoint_path):
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        state_dict = checkpoint.get("model", checkpoint)
        
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        
        print(f"âœ… åŠ è½½é¢„è®­ç»ƒæƒé‡: {checkpoint_path}")
        if missing_keys:
            print(f"âš ï¸ ç¼ºå¤±çš„é”® ({len(missing_keys)}): {missing_keys[:3]}...")
        if unexpected_keys:
            print(f"âš ï¸ å¤šä½™çš„é”® ({len(unexpected_keys)}): {unexpected_keys[:3]}...")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {checkpoint_path}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
    
    return model


def setup_lora(model, lora_config):
    """ä¸º Q-Former æ·»åŠ  LoRA é€‚é…å™¨"""
    print(f"\nğŸ”§ é…ç½® LoRA...")
    
    peft_config = LoraConfig(
        task_type=TaskType.FEATURE_EXTRACTION,
        inference_mode=False,
        r=lora_config['rank'],
        lora_alpha=lora_config['alpha'],
        lora_dropout=lora_config['dropout'],
        target_modules=lora_config['target_modules'],
        bias="none",
    )
    
    model.Qformer = get_peft_model(model.Qformer, peft_config)
    
    print(f"  Rank: {lora_config['rank']}")
    print(f"  Alpha: {lora_config['alpha']}")
    print(f"  Target modules: {lora_config['target_modules']}")
    
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"\nğŸ’¾ å‚æ•°ç»Ÿè®¡:")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    
    return model


def get_lora_config(profile='balanced'):
    """é¢„å®šä¹‰çš„ LoRA é…ç½®"""
    configs = {
        'minimal': {
            'rank': 4,
            'alpha': 8,
            'dropout': 0.1,
            'target_modules': ['query', 'value'],
        },
        'balanced': {
            'rank': 8,
            'alpha': 16,
            'dropout': 0.05,
            'target_modules': ['query', 'key', 'value'],
        },
        'full': {
            'rank': 16,
            'alpha': 32,
            'dropout': 0.05,
            'target_modules': ['query', 'key', 'value', 'dense'],
        }
    }
    return configs.get(profile, configs['balanced'])


# ============================================
# Early Stopping
# ============================================
class EarlyStopping:
    """Early Stopping æœºåˆ¶"""
    def __init__(self, patience=5, min_delta=0, mode='min'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        
    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
            return True
        
        if self.mode == 'min':
            improved = score < (self.best_score - self.min_delta)
        else:
            improved = score > (self.best_score + self.min_delta)
        
        if improved:
            self.best_score = score
            self.counter = 0
            return True
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
            return False


# ============================================
# è®­ç»ƒä¸€ä¸ª Epochï¼ˆITC + Sceneï¼‰
# ============================================
def train_one_epoch(model, scene_head, train_loader, optimizer, scheduler, scaler, 
                     device, epoch, writer, global_step, loss_weights):
    """è®­ç»ƒä¸€ä¸ª epochï¼ˆä½¿ç”¨ ITC + Scene è”åˆæŸå¤±ï¼‰"""
    model.train()
    model.visual_encoder.eval()  # ä¿æŒ ViT å†»ç»“
    scene_head.train()
    
    total_loss = 0
    total_itc_loss = 0
    total_scene_loss = 0
    total_acc_i2t = 0
    total_acc_t2i = 0
    total_scene_acc = 0
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
    
    for batch_idx, samples in enumerate(pbar):
        images = samples['image'].to(device)
        texts = samples['text']
        scene_labels = samples['scene_label'].to(device)
        
        with autocast():
            # ========== ITC Loss ==========
            image_feats, text_feats = model({'image': images, 'text': texts})
            
            itc_loss, acc_i2t, acc_t2i = compute_itc_loss(
                image_feats, text_feats, model.temp, device
            )
            
            # ========== Scene Classification Loss ==========
            scene_loss, scene_acc = compute_scene_loss(
                scene_head, image_feats, scene_labels, device
            )
            
            # ========== æ€»æŸå¤± ==========
            loss = (
                loss_weights['itc'] * itc_loss + 
                loss_weights['scene'] * scene_loss
            )
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        total_itc_loss += itc_loss.item()
        total_scene_loss += scene_loss.item()
        total_acc_i2t += acc_i2t
        total_acc_t2i += acc_t2i
        total_scene_acc += scene_acc
        
        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'itc': f'{itc_loss.item():.4f}',
            'scene': f'{scene_loss.item():.4f}',
            'i2t': f'{acc_i2t:.2%}',
            'scene_acc': f'{scene_acc:.2%}',
            'lr': f'{optimizer.param_groups[0]["lr"]:.2e}'
        })
        
        # è®°å½•åˆ° TensorBoard
        if batch_idx % 10 == 0:
            writer.add_scalar('Train/total_loss', loss.item(), global_step[0])
            writer.add_scalar('Train/itc_loss', itc_loss.item(), global_step[0])
            writer.add_scalar('Train/scene_loss', scene_loss.item(), global_step[0])
            writer.add_scalar('Train/acc_i2t', acc_i2t, global_step[0])
            writer.add_scalar('Train/acc_t2i', acc_t2i, global_step[0])
            writer.add_scalar('Train/scene_acc', scene_acc, global_step[0])
            writer.add_scalar('Train/lr', optimizer.param_groups[0]['lr'], global_step[0])
        
        global_step[0] += 1
    
    n = len(train_loader)
    return {
        'loss': total_loss / n,
        'itc_loss': total_itc_loss / n,
        'scene_loss': total_scene_loss / n,
        'acc_i2t': total_acc_i2t / n,
        'acc_t2i': total_acc_t2i / n,
        'scene_acc': total_scene_acc / n,
    }


# ============================================
# éªŒè¯ï¼ˆITC + Sceneï¼‰
# ============================================
def validate(model, scene_head, val_loader, device, epoch, writer, loss_weights):
    """éªŒè¯ï¼ˆä½¿ç”¨ ITC + Sceneï¼‰"""
    model.eval()
    scene_head.eval()
    
    total_loss = 0
    total_itc_loss = 0
    total_scene_loss = 0
    total_acc_i2t = 0
    total_acc_t2i = 0
    total_scene_acc = 0
    
    # ç”¨äºè®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    scene_correct = torch.zeros(NUM_SCENE_CLASSES)
    scene_total = torch.zeros(NUM_SCENE_CLASSES)
    
    with torch.no_grad():
        for samples in tqdm(val_loader, desc="Validating"):
            images = samples['image'].to(device)
            texts = samples['text']
            scene_labels = samples['scene_label'].to(device)
            
            with autocast():
                # ITC
                image_feats, text_feats = model({'image': images, 'text': texts})
                itc_loss, acc_i2t, acc_t2i = compute_itc_loss(
                    image_feats, text_feats, model.temp, device
                )
                
                # Scene
                scene_loss, scene_acc = compute_scene_loss(
                    scene_head, image_feats, scene_labels, device
                )
                
                loss = loss_weights['itc'] * itc_loss + loss_weights['scene'] * scene_loss
            
            total_loss += loss.item()
            total_itc_loss += itc_loss.item()
            total_scene_loss += scene_loss.item()
            total_acc_i2t += acc_i2t
            total_acc_t2i += acc_t2i
            total_scene_acc += scene_acc
            
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
            if image_feats.dim() == 3:
                image_feats_2d = image_feats.mean(dim=1)
            else:
                image_feats_2d = image_feats
            logits = scene_head(image_feats_2d)
            pred = logits.argmax(dim=1)
            
            for i in range(len(scene_labels)):
                label = scene_labels[i].item()
                scene_total[label] += 1
                if pred[i] == scene_labels[i]:
                    scene_correct[label] += 1
    
    n = len(val_loader)
    results = {
        'loss': total_loss / n,
        'itc_loss': total_itc_loss / n,
        'scene_loss': total_scene_loss / n,
        'acc_i2t': total_acc_i2t / n,
        'acc_t2i': total_acc_t2i / n,
        'scene_acc': total_scene_acc / n,
    }
    
    # è®°å½•åˆ° TensorBoard
    writer.add_scalar('Val/total_loss', results['loss'], epoch)
    writer.add_scalar('Val/itc_loss', results['itc_loss'], epoch)
    writer.add_scalar('Val/scene_loss', results['scene_loss'], epoch)
    writer.add_scalar('Val/acc_i2t', results['acc_i2t'], epoch)
    writer.add_scalar('Val/scene_acc', results['scene_acc'], epoch)
    
    print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
    print(f"  Total Loss: {results['loss']:.4f}")
    print(f"  ITC Loss: {results['itc_loss']:.4f} | I2T Acc: {results['acc_i2t']:.2%}")
    print(f"  Scene Loss: {results['scene_loss']:.4f} | Scene Acc: {results['scene_acc']:.2%}")
    
    # æ‰“å°æ¯ä¸ªåœºæ™¯çš„å‡†ç¡®ç‡
    print(f"\nğŸ“ˆ å„åœºæ™¯åˆ†ç±»å‡†ç¡®ç‡:")
    for i in range(NUM_SCENE_CLASSES):
        if scene_total[i] > 0:
            acc = scene_correct[i] / scene_total[i]
            print(f"  {ID_TO_SCENE[i]:8s}: {acc:.2%} ({int(scene_correct[i])}/{int(scene_total[i])})")
    
    return results


# ============================================
# ä¿å­˜æ£€æŸ¥ç‚¹
# ============================================
def save_checkpoint(model, scene_head, optimizer, scheduler, epoch, loss, output_dir, is_best=False):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    os.makedirs(output_dir, exist_ok=True)
    
    lora_dir = os.path.join(output_dir, f'checkpoint_epoch_{epoch}')
    os.makedirs(lora_dir, exist_ok=True)
    
    # ä¿å­˜ LoRA æƒé‡
    model.Qformer.save_pretrained(lora_dir)
    
    # ä¿å­˜åœºæ™¯åˆ†ç±»å¤´
    torch.save(scene_head.state_dict(), os.path.join(lora_dir, 'scene_head.pth'))
    
    # ä¿å­˜è®­ç»ƒçŠ¶æ€
    state = {
        'epoch': epoch,
        'loss': loss,
        'optimizer': optimizer.state_dict(),
        'scheduler': scheduler.state_dict(),
    }
    torch.save(state, os.path.join(lora_dir, 'training_state.pth'))
    
    print(f"âœ… ä¿å­˜æ£€æŸ¥ç‚¹: {lora_dir}")
    
    if is_best:
        best_dir = os.path.join(output_dir, 'best_model')
        os.makedirs(best_dir, exist_ok=True)
        model.Qformer.save_pretrained(best_dir)
        torch.save(scene_head.state_dict(), os.path.join(best_dir, 'scene_head.pth'))
        torch.save(state, os.path.join(best_dir, 'training_state.pth'))
        print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹: {best_dir}")


# ============================================
# ä¸»è®­ç»ƒå‡½æ•°
# ============================================
def main():
    config = {
        # æ•°æ®è·¯å¾„
        'annotation_file': '/workspace/vlm/lab/output/scene_annotations.json',
        'image_dir': '/data/fasion/train/image',
        
        # æ•°æ®åˆ’åˆ†ï¼ˆé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨åˆ’åˆ†ï¼‰
        'train_file': '/workspace/vlm/lab/output/train_split.json',
        'val_file': '/workspace/vlm/lab/output/val_split.json',
        'test_file': '/workspace/vlm/lab/output/test_split.json',
        
        # è®­ç»ƒå‚æ•°
        'batch_size': 32,
        'num_workers': 8,
        'use_key_features': False,  # True: ä½¿ç”¨ key_features, False: ä½¿ç”¨ text
        
        # æ¨¡å‹
        'checkpoint': 'checkpoint_04.pth',
        'lora_profile': 'full',  # minimal / balanced / full
        
        # è®­ç»ƒ
        'epochs': 30,
        'lr': 1e-4,
        'weight_decay': 0.05,
        'warmup_epochs': 2,
        
        # æŸå¤±æƒé‡
        'loss_weights': {
            'itc': 0.2,    # ITC æŸå¤±æƒé‡
            'scene': 0.8,  # åœºæ™¯åˆ†ç±»æŸå¤±æƒé‡
        },
        
        # Early stopping
        'patience': 30,
        'min_delta': 0.0,
        
        # è¾“å‡º
        'output_dir': 'outputs/fashion_lora_itc_scene',
        'log_dir': 'runs/fashion_lora_itc_scene',
        
        'device': 'cuda:4' ,
    }
    
    print("="*60)
    print("ğŸš€ BLIP2 Q-Former LoRA å¾®è°ƒ - æ—¶å°šåœºæ™¯åˆ†ç±»")
    print("="*60)
    print(f"\nâš™ï¸ é…ç½®:")
    for key, value in config.items():
        if not key.endswith('_file') and not key.endswith('_dir'):
            print(f"  {key}: {value}")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(config['annotation_file']):
        raise FileNotFoundError(f"âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {config['annotation_file']}")
    
    if not os.path.exists(config['image_dir']):
        raise FileNotFoundError(f"âŒ å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {config['image_dir']}")
    
    # æ•°æ®åˆ’åˆ†ï¼ˆå¦‚æœåˆ†å‰²æ–‡ä»¶ä¸å­˜åœ¨ï¼‰
    if not all(os.path.exists(f) for f in [config['train_file'], config['val_file'], config['test_file']]):
        train_file, val_file, test_file = split_dataset(
            config['annotation_file'],
            train_ratio=0.8,
            val_ratio=0.1,
            seed=42
        )
        config['train_file'] = train_file
        config['val_file'] = val_file
        config['test_file'] = test_file
    
    # å‡†å¤‡æ•°æ®é›†
    print(f"\nğŸ“Š åŠ è½½æ•°æ®é›†...")
    print(f"  ä½¿ç”¨æ–‡æœ¬å­—æ®µ: {'key_features' if config['use_key_features'] else 'text'}")
    
    train_dataset = FashionSceneDataset(
        config['train_file'],
        config['image_dir'],
        transform=get_train_transform(),
        use_key_features=config['use_key_features']
    )
    
    val_dataset = FashionSceneDataset(
        config['val_file'],
        config['image_dir'],
        transform=get_val_transform(),
        use_key_features=config['use_key_features']
    )
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True,
        num_workers=config['num_workers'], 
        pin_memory=True,
        collate_fn=lambda x: {
            'image': torch.stack([item['image'] for item in x]),
            'text': [item['text'] for item in x],
            'scene_label': torch.tensor([item['scene_label'] for item in x], dtype=torch.long),
        }
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config['batch_size'], 
        shuffle=False,
        num_workers=config['num_workers'], 
        pin_memory=True,
        collate_fn=lambda x: {
            'image': torch.stack([item['image'] for item in x]),
            'text': [item['text'] for item in x],
            'scene_label': torch.tensor([item['scene_label'] for item in x], dtype=torch.long),
        }
    )
    
    # å‡†å¤‡æ¨¡å‹
    model = load_base_model(config['checkpoint'])
    lora_config = get_lora_config(config['lora_profile'])
    model = setup_lora(model, lora_config)
    model.to(config['device'])
    
    # åœºæ™¯åˆ†ç±»å¤´
    scene_head = SceneClassificationHead(
        input_dim=256,
        num_classes=NUM_SCENE_CLASSES,
        dropout=0.1
    ).to(config['device'])
    
    print(f"\nğŸ¯ åœºæ™¯åˆ†ç±»å¤´:")
    print(f"  è¾“å…¥ç»´åº¦: 256")
    print(f"  ç±»åˆ«æ•°: {NUM_SCENE_CLASSES}")
    print(f"  å‚æ•°é‡: {sum(p.numel() for p in scene_head.parameters()):,}")
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        list(filter(lambda p: p.requires_grad, model.parameters())) + 
        list(scene_head.parameters()),
        lr=config['lr'], 
        weight_decay=config['weight_decay']
    )
    
    total_steps = len(train_loader) * config['epochs']
    warmup_steps = len(train_loader) * config['warmup_epochs']
    scheduler = CosineAnnealingLR(optimizer, T_max=total_steps - warmup_steps, eta_min=1e-6)
    
    scaler = GradScaler()
    early_stopping = EarlyStopping(patience=config['patience'], min_delta=config['min_delta'])
    writer = SummaryWriter(log_dir=config['log_dir'])
    
    # è®­ç»ƒ
    print(f"\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
    print(f"  æ€» Epochs: {config['epochs']}")
    print(f"  è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)} batches/epoch")
    print(f"  éªŒè¯æ‰¹æ¬¡: {len(val_loader)} batches/epoch")
    
    global_step = [0]
    best_val_loss = float('inf')
    start_time = time.time()
    
    for epoch in range(1, config['epochs'] + 1):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch}/{config['epochs']}")
        print(f"{'='*60}")
        
        train_results = train_one_epoch(
            model, scene_head, train_loader, optimizer, scheduler, scaler,
            config['device'], epoch, writer, global_step, config['loss_weights']
        )
        
        print(f"\nğŸ“ˆ è®­ç»ƒç»“æœ:")
        print(f"  Total Loss: {train_results['loss']:.4f}")
        print(f"  ITC Loss: {train_results['itc_loss']:.4f} | I2T: {train_results['acc_i2t']:.2%}")
        print(f"  Scene Loss: {train_results['scene_loss']:.4f} | Acc: {train_results['scene_acc']:.2%}")
        
        val_results = validate(
            model, scene_head, val_loader, config['device'], epoch, writer, config['loss_weights']
        )
        
        is_best = early_stopping(val_results['loss'])
        if is_best:
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! (Val Loss: {val_results['loss']:.4f})")
            best_val_loss = val_results['loss']
        
        if epoch % 5 == 0 or is_best:
            save_checkpoint(
                model, scene_head, optimizer, scheduler, epoch, 
                val_results['loss'], config['output_dir'], is_best
            )
        
        if early_stopping.early_stop:
            print(f"\nâ¹ï¸ Early Stopping è§¦å‘! (è¿ç»­ {config['patience']} ä¸ª epoch æ— æ”¹å–„)")
            break
    
    elapsed_time = time.time() - start_time
    print(f"\n{'='*60}")
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"  æ€»æ—¶é•¿: {elapsed_time/3600:.2f} å°æ—¶")
    print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  æ¨¡å‹æƒé‡: {config['output_dir']}")
    print(f"  è®­ç»ƒæ—¥å¿—: {config['log_dir']}")
    print(f"\nğŸ“Š æŸ¥çœ‹æ—¥å¿—:")
    print(f"  tensorboard --logdir={config['log_dir']}")
    print(f'  "$BROWSER" http://localhost:6006')
    
    writer.close()


if __name__ == "__main__":
    main()