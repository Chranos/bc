import os
import torch
import torch.nn as nn
from modelscope.models.base import TorchModel
from modelscope.preprocessors.base import Preprocessor
from modelscope.pipelines.base import Model, Pipeline
from modelscope.utils.config import Config
from modelscope.pipelines.builder import PIPELINES
from modelscope.preprocessors.builder import PREPROCESSORS
from modelscope.models.builder import MODELS

from models.blip2_qformer import Blip2Qformer as BLIP2
from models.blip2 import Blip2Base
from PIL import Image
from torchvision import transforms
from torchvision.transforms import InterpolationMode
import requests


# æ³¨å†Œå›¾åƒæè¿°ç”Ÿæˆæ¨¡å‹
@MODELS.register_module('image-captioning', module_name='BLIP2_Caption')
class ImageCaptioningModel(TorchModel):

    def __init__(self, model_dir, *args, **kwargs):
        super().__init__(model_dir, *args, **kwargs)
        self.device = kwargs.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self.init_model(**kwargs)
        self.model.to(self.device)
        
        # åˆå§‹åŒ– tokenizer ç”¨äºè§£ç ç”Ÿæˆçš„æ–‡æœ¬
        self.tokenizer = Blip2Base.init_tokenizer()
        print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°: {self.device}")

    def forward(self, input_tensor, **forward_params):
        """ç”Ÿæˆå›¾åƒæè¿°"""
        img_inputs = input_tensor['img_inputs']
        
        # æå–ç”Ÿæˆå‚æ•°ï¼ˆåªä½¿ç”¨æ¨¡å‹æ”¯æŒçš„å‚æ•°ï¼‰
        max_length = forward_params.get('max_length', 30)
        min_length = forward_params.get('min_length', 10)
        num_beams = forward_params.get('num_beams', 3)
        top_p = forward_params.get('top_p', 0.9)
        repetition_penalty = forward_params.get('repetition_penalty', 1.0)
        use_nucleus_sampling = forward_params.get('use_nucleus_sampling', False)
        
        captions = []
        with torch.no_grad():
            for img_input in img_inputs:
                img_input = img_input.to(self.device)
                
                # è°ƒç”¨æ¨¡å‹çš„ generate æ–¹æ³•ç”Ÿæˆæè¿°
                # BLIP2 æ¨¡å‹éœ€è¦è¾“å…¥å­—å…¸æ ¼å¼
                samples = {"image": img_input}
                
                # ä½¿ç”¨æ¨¡å‹çš„ç”Ÿæˆæ–¹æ³•ï¼ˆåªä¼ é€’æ”¯æŒçš„å‚æ•°ï¼‰
                output_ids = self.model.generate(
                    samples,
                    use_nucleus_sampling=use_nucleus_sampling,
                    num_beams=num_beams,
                    max_length=max_length,
                    min_length=min_length,
                    top_p=top_p,
                    repetition_penalty=repetition_penalty
                )
                
                # è§£ç ç”Ÿæˆçš„ token IDs
                if isinstance(output_ids, torch.Tensor):
                    # å¦‚æœè¿”å›çš„æ˜¯å¼ é‡
                    caption = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
                elif isinstance(output_ids, list):
                    # å¦‚æœè¿”å›çš„æ˜¯åˆ—è¡¨ï¼ˆBLIP2é€šå¸¸è¿”å›åˆ—è¡¨ï¼‰
                    caption = output_ids[0] if output_ids else "æ— æ³•ç”Ÿæˆæè¿°"
                else:
                    caption = str(output_ids)
                
                captions.append(caption.strip())
        
        return captions

    def init_model(self, **kwargs):
        """åˆå§‹åŒ–æ¨¡å‹å¹¶åŠ è½½æƒé‡"""
        weight_path = kwargs.get('weight_path')

        if not os.path.isfile(weight_path):
            weight_path = os.path.join(self.model_dir, weight_path)
        
        # åˆ›å»º BLIP2 æ¨¡å‹
        model = BLIP2()
        checkpoint = torch.load(weight_path, map_location='cpu')

        if "model" in checkpoint.keys():
            state_dict = checkpoint["model"]
        else:
            state_dict = checkpoint
        
        model.load_state_dict(state_dict, strict=False)
        model.eval()
        return model


# æ³¨å†Œå›¾åƒæè¿°é¢„å¤„ç†å™¨
@PREPROCESSORS.register_module('multi-modal', module_name='caption-preprocessor')
class CaptionPreprocessor(Preprocessor):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # å›¾åƒé¢„å¤„ç†ï¼ˆä¸ BLIP2 è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        img_size = 224
        normalize = transforms.Normalize(
            (0.48145466, 0.4578275, 0.40821073), 
            (0.26862954, 0.26130258, 0.27577711)
        )
        self.transform = transforms.Compose([
            transforms.Resize((img_size, img_size), interpolation=InterpolationMode.BICUBIC),
            transforms.ToTensor(),
            normalize,
        ])

    def __call__(self, input):
        """é¢„å¤„ç†è¾“å…¥å›¾ç‰‡"""
        # æ”¯æŒå•å¼ å›¾ç‰‡æˆ–å›¾ç‰‡åˆ—è¡¨
        if isinstance(input, str):
            images = [input]
        elif isinstance(input, list):
            images = input
        elif isinstance(input, dict) and 'img' in input:
            images = input['img'] if isinstance(input['img'], list) else [input['img']]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥æ ¼å¼: {type(input)}")
        
        # å¤„ç†æ¯å¼ å›¾ç‰‡
        image_inputs = []
        for img in images:
            if isinstance(img, str):
                # æ”¯æŒ URL æˆ–æœ¬åœ°è·¯å¾„
                if img.startswith("https") or img.startswith("http"):
                    image = Image.open(requests.get(img, stream=True).raw).convert('RGB')
                else:
                    image = Image.open(img).convert('RGB')
            elif isinstance(img, Image.Image):
                image = img.convert('RGB')
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å›¾ç‰‡æ ¼å¼: {type(img)}")
            
            # åº”ç”¨å˜æ¢
            image = self.transform(image)  # [3, 224, 224]
            image = image.unsqueeze(0)     # [1, 3, 224, 224]
            image_inputs.append(image)
        
        return {'img_inputs': image_inputs}


# æ³¨å†Œå›¾åƒæè¿°ç”Ÿæˆç®¡é“
@PIPELINES.register_module('image-captioning', module_name='BLIP2-Caption')
class ImageCaptioningPipeline(Pipeline):

    def __init__(self, model, preprocessor=None, **kwargs):
        """åˆå§‹åŒ–å›¾åƒæè¿°ç”Ÿæˆç®¡é“"""
        assert isinstance(model, str) or isinstance(model, Model), \
            'model must be a single str or Model'

        if isinstance(model, str):
            pipe_model = Model.from_pretrained(model, **kwargs)
        elif isinstance(model, Model):
            pipe_model = model
        else:
            raise NotImplementedError
        
        pipe_model.eval()
        
        if preprocessor is None:
            preprocessor = CaptionPreprocessor()

        super().__init__(model=pipe_model, preprocessor=preprocessor, **kwargs)

    def _sanitize_parameters(self, **pipeline_parameters):
        """åˆ†ç¦»é¢„å¤„ç†ã€å‰å‘å’Œåå¤„ç†å‚æ•°"""
        # ç”Ÿæˆå‚æ•°ä¼ é€’ç»™ forwardï¼ˆåªä¼ é€’æ”¯æŒçš„å‚æ•°ï¼‰
        forward_params = {}
        supported_params = ['max_length', 'min_length', 'num_beams', 
                           'top_p', 'repetition_penalty', 'use_nucleus_sampling']
        
        for key in supported_params:
            if key in pipeline_parameters:
                forward_params[key] = pipeline_parameters[key]
        
        return {}, forward_params, {}

    def _check_input(self, inputs):
        pass

    def _check_output(self, outputs):
        pass

    def forward(self, inputs, **forward_params):
        """æ‰§è¡Œå‰å‘æ¨ç†"""
        return super().forward(inputs, **forward_params)

    def postprocess(self, inputs):
        """åå¤„ç†ï¼šæ ¼å¼åŒ–è¾“å‡º"""
        return inputs


# é…ç½®æ–‡ä»¶
usr_config_path = '.'
config = Config({
    'framework': 'pytorch',
    'task': 'image-captioning',
    "model": {
        "type": "BLIP2_Caption",
        "weight_path": "checkpoint_04.pth",
        "half": False
    },
    "pipeline": {"type": "BLIP2-Caption"}
})
config.dump('.' + '/configuration.json')


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    from modelscope.pipelines import pipeline
    
    print("\n" + "="*60)
    print("ğŸ–¼ï¸  BLIP2 ä¸­æ–‡å›¾åƒæè¿°ç”Ÿæˆ")
    print("="*60 + "\n")
    
    # åˆ›å»ºæ¨ç†ç®¡é“
    caption_pipeline = pipeline(
        'image-captioning', 
        model=usr_config_path,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    

    image_paths = ["test1.jpg", "test3.jpg"]
    existing_images = [img for img in image_paths if os.path.exists(img)]
    input_dict = {'img': existing_images if existing_images else ["test1.jpg"]}


    
    if existing_images:
        # ä½¿ç”¨ beam searchï¼ˆæ›´å‡†ç¡®ï¼‰
        captions_beam = caption_pipeline(
            existing_images[1],
            num_beams=5,
            max_length=30,
            repetition_penalty=1.2
        )
        print(f"ğŸ“· å›¾ç‰‡: {existing_images[1]}")
        print(f"ğŸ“ Beam Search: {captions_beam[0]}")
        

    
    print("\n" + "="*60)
    print("âœ… æµ‹è¯•å®Œæˆ")
    print("="*60 + "\n")