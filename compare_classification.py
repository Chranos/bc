#!/usr/bin/env python3
"""
åœºæ™¯åˆ†ç±»å‡†ç¡®ç‡å¯¹æ¯”å®éªŒè„šæœ¬
æ”¯æŒå¯¹æ¯”å¤šä¸ªæ¨¡å‹çš„åˆ†ç±»æ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š
- BLIP2 (æœªå¾®è°ƒåŸºçº¿ - é›¶æ ·æœ¬)
- BLIP2 + LoRA (å¾®è°ƒå)
- CLIP ç³»åˆ— (é›¶æ ·æœ¬ & å¾®è°ƒ)
- ResNet-50 (é¢„è®­ç»ƒ & å¾®è°ƒ)
- ViT-Base (é¢„è®­ç»ƒ & å¾®è°ƒ)

è¾“å‡ºè¯¦ç»†çš„å¯¹æ¯”è¡¨æ ¼ã€æ··æ·†çŸ©é˜µå’Œå¯è§†åŒ–å›¾è¡¨
"""

import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from torchvision import transforms
from tqdm import tqdm
import numpy as np
from sklearn.metrics import (
    classification_report, 
    confusion_matrix,
    accuracy_score,
    precision_recall_fscore_support
)
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from typing import Dict, List, Tuple, Optional
import warnings

warnings.filterwarnings('ignore')

from models.model_zoo import create_model


# ============================================
# åœºæ™¯ç±»åˆ«å®šä¹‰
# ============================================
SCENE_CATEGORIES = [
    'èŒåœºæ­£è£…', 'èŒåœºä¼‘é—²', 'è¿åŠ¨å¥èº«', 'æˆ·å¤–æ¢é™©', 'å±…å®¶ä¼‘é—²',
    'ç¤¾äº¤èšä¼š', 'æ—…è¡Œåº¦å‡', 'è¿åŠ¨èµ›äº‹', 'å©šç¤¼ç›¸å…³', 'ç‰¹æ®ŠåŠŸèƒ½',
]
SCENE_TO_ID = {name: idx for idx, name in enumerate(SCENE_CATEGORIES)}
ID_TO_SCENE = {idx: name for idx, name in enumerate(SCENE_CATEGORIES)}
NUM_SCENE_CLASSES = len(SCENE_CATEGORIES)


# ============================================
# ç”Ÿæˆåœºæ™¯æè¿°æ–‡æœ¬ï¼ˆç”¨äºé›¶æ ·æœ¬åˆ†ç±»ï¼‰
# ============================================
def generate_scene_prompts(template_style='descriptive'):
    """
    ç”Ÿæˆåœºæ™¯ç±»åˆ«çš„æ–‡æœ¬æè¿°
    
    Args:
        template_style: æ¨¡æ¿é£æ ¼
            - 'simple': ç®€å•ç±»åˆ«å
            - 'descriptive': æè¿°æ€§æ–‡æœ¬
            - 'context': å¸¦åœºæ™¯ä¸Šä¸‹æ–‡
    
    Returns:
        prompts: æ–‡æœ¬æè¿°åˆ—è¡¨
    """
    if template_style == 'simple':
        # ç®€å•ç±»åˆ«å
        prompts = SCENE_CATEGORIES
    
    elif template_style == 'descriptive':
        # æè¿°æ€§æ–‡æœ¬
        prompts = [
            'é€‚åˆèŒåœºæ­£å¼åœºåˆçš„æ­£è£…æœé¥°',
            'é€‚åˆèŒåœºçš„å•†åŠ¡ä¼‘é—²æœè£…',
            'é€‚åˆè¿åŠ¨å¥èº«çš„è¿åŠ¨æœè£…',
            'é€‚åˆæˆ·å¤–æ¢é™©çš„åŠŸèƒ½æ€§æœè£…',
            'é€‚åˆå®¶ä¸­ç©¿ç€çš„å±…å®¶ä¼‘é—²æœ',
            'é€‚åˆç¤¾äº¤èšä¼šçš„æ—¶å°šæœè£…',
            'é€‚åˆæ—…è¡Œåº¦å‡çš„è½»ä¾¿æœè£…',
            'é€‚åˆè¿åŠ¨èµ›äº‹çš„ä¸“ä¸šè£…å¤‡',
            'é€‚åˆå©šç¤¼åœºåˆçš„ç¤¼æœ',
            'å…·æœ‰ç‰¹æ®ŠåŠŸèƒ½çš„æœè£…',
        ]
    
    elif template_style == 'context':
        # å¸¦åœºæ™¯ä¸Šä¸‹æ–‡
        prompts = [
            'ä¸€å¼ å±•ç¤ºèŒåœºæ­£è£…æœé¥°çš„å›¾ç‰‡',
            'ä¸€å¼ å±•ç¤ºèŒåœºä¼‘é—²æœè£…çš„å›¾ç‰‡',
            'ä¸€å¼ å±•ç¤ºè¿åŠ¨å¥èº«æœè£…çš„å›¾ç‰‡',
            'ä¸€å¼ å±•ç¤ºæˆ·å¤–æ¢é™©æœè£…çš„å›¾ç‰‡',
            'ä¸€å¼ å±•ç¤ºå±…å®¶ä¼‘é—²æœçš„å›¾ç‰‡',
            'ä¸€å¼ å±•ç¤ºç¤¾äº¤èšä¼šæœè£…çš„å›¾ç‰‡',
            'ä¸€å¼ å±•ç¤ºæ—…è¡Œåº¦å‡æœè£…çš„å›¾ç‰‡',
            'ä¸€å¼ å±•ç¤ºè¿åŠ¨èµ›äº‹è£…å¤‡çš„å›¾ç‰‡',
            'ä¸€å¼ å±•ç¤ºå©šç¤¼ç¤¼æœçš„å›¾ç‰‡',
            'ä¸€å¼ å±•ç¤ºç‰¹æ®ŠåŠŸèƒ½æœè£…çš„å›¾ç‰‡',
        ]
    
    else:
        raise ValueError(f"Unknown template style: {template_style}")
    
    print(f"  ğŸ“ åœºæ™¯æè¿°æ¨¡æ¿: {template_style}")
    print(f"  ç¤ºä¾‹: {prompts[0]}")
    
    return prompts


# ============================================
# é›¶æ ·æœ¬åˆ†ç±»å™¨ï¼ˆç”¨äºæœªå¾®è°ƒçš„æ¨¡å‹ï¼‰
# ============================================
class ZeroShotClassifier:
    """
    é›¶æ ·æœ¬åˆ†ç±»å™¨ - ç”¨äºæ²¡æœ‰åˆ†ç±»å¤´çš„æ¨¡å‹
    é€šè¿‡å›¾æ–‡ç›¸ä¼¼åº¦è¿›è¡Œåˆ†ç±»
    """
    def __init__(self, model, scene_prompts: List[str], device='cuda'):
        """
        Args:
            model: æ¨¡å‹åŒ…è£…å™¨
            scene_prompts: åœºæ™¯ç±»åˆ«çš„æ–‡æœ¬æè¿°
            device: è®¾å¤‡
        """
        self.model = model
        self.device = device
        
        # é¢„è®¡ç®—åœºæ™¯æ–‡æœ¬ç‰¹å¾
        print(f"  ğŸ”„ é¢„è®¡ç®—åœºæ™¯æ–‡æœ¬ç‰¹å¾...")
        
        # åˆ›å»ºä¸€ä¸ª dummy å›¾åƒ
        dummy_images = [Image.new('RGB', (224, 224), color=(128, 128, 128))] * len(scene_prompts)
        
        try:
            _, text_feats = model.extract_features(dummy_images, scene_prompts)
            
            if text_feats is None:
                raise ValueError("æ¨¡å‹ä¸æ”¯æŒæ–‡æœ¬ç‰¹å¾æå–")
            
            self.scene_text_feats = F.normalize(text_feats, dim=-1)
            print(f"  âœ… åœºæ™¯æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {self.scene_text_feats.shape}")
        
        except Exception as e:
            print(f"  âŒ æ–‡æœ¬ç‰¹å¾æå–å¤±è´¥: {e}")
            raise
    
    @torch.no_grad()
    def predict(self, images):
        """
        é›¶æ ·æœ¬é¢„æµ‹
        
        Args:
            images: PIL Images åˆ—è¡¨
        
        Returns:
            logits: [B, num_classes]
        """
        # æå–å›¾åƒç‰¹å¾
        image_feats, _ = self.model.extract_features(images, [""] * len(images))
        
        # å½’ä¸€åŒ–
        image_feats = F.normalize(image_feats, dim=-1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦ä½œä¸º logits
        logits = image_feats @ self.scene_text_feats.t()
        
        # ç¼©æ”¾åˆ°æ›´åˆç†çš„èŒƒå›´
        logits = logits * 100.0
        
        return logits


# ============================================
# æ‰©å±•çš„æ¨¡å‹åŒ…è£…å™¨ï¼ˆæ”¯æŒé›¶æ ·æœ¬åˆ†ç±»ï¼‰
# ============================================
class ExtendedModelWrapper:
    """
    æ‰©å±•çš„æ¨¡å‹åŒ…è£…å™¨ - ä¸ºæ²¡æœ‰åˆ†ç±»å¤´çš„æ¨¡å‹æ·»åŠ é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›
    """
    def __init__(self, base_model, use_zero_shot=False, scene_prompts=None):
        """
        Args:
            base_model: åŸå§‹æ¨¡å‹åŒ…è£…å™¨
            use_zero_shot: æ˜¯å¦ä½¿ç”¨é›¶æ ·æœ¬åˆ†ç±»
            scene_prompts: åœºæ™¯ç±»åˆ«çš„æ–‡æœ¬æè¿°ï¼ˆç”¨äºé›¶æ ·æœ¬ï¼‰
        """
        self.base_model = base_model
        self.use_zero_shot = use_zero_shot
        self.zero_shot_classifier = None
        
        if use_zero_shot and scene_prompts:
            print(f"  ğŸ¯ åˆå§‹åŒ–é›¶æ ·æœ¬åˆ†ç±»å™¨...")
            try:
                self.zero_shot_classifier = ZeroShotClassifier(
                    base_model, 
                    scene_prompts,
                    base_model.device
                )
            except Exception as e:
                print(f"  âš ï¸ é›¶æ ·æœ¬åˆ†ç±»å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.zero_shot_classifier = None
    
    def extract_features(self, images, texts):
        """æå–ç‰¹å¾"""
        return self.base_model.extract_features(images, texts)
    
    def classify_scene(self, images):
        """åœºæ™¯åˆ†ç±»"""
        # ä¼˜å…ˆä½¿ç”¨åŸç”Ÿåˆ†ç±»å¤´
        try:
            native_logits = self.base_model.classify_scene(images)
            if native_logits is not None:
                return native_logits
        except Exception as e:
            print(f"  âš ï¸ åŸç”Ÿåˆ†ç±»å¤±è´¥: {e}")
        
        # å¦åˆ™ä½¿ç”¨é›¶æ ·æœ¬åˆ†ç±»
        if self.use_zero_shot and self.zero_shot_classifier:
            return self.zero_shot_classifier.predict(images)
        
        return None
    
    def get_model_info(self):
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        info = self.base_model.get_model_info()
        if self.use_zero_shot and self.zero_shot_classifier:
            info['classification_method'] = 'zero-shot'
        elif 'classification_method' not in info:
            info['classification_method'] = 'supervised'
        return info
    
    @property
    def model_name(self):
        return getattr(self.base_model, 'model_name', 'Unknown')
    
    @property
    def device(self):
        return self.base_model.device


# ============================================
# æµ‹è¯•æ•°æ®é›†
# ============================================
class ClassificationTestDataset(Dataset):
    """åœºæ™¯åˆ†ç±»æµ‹è¯•æ•°æ®é›†"""
    
    def __init__(self, annotation_file: str, image_dir: str, transform=None):
        """
        Args:
            annotation_file: æ ‡æ³¨æ–‡ä»¶è·¯å¾„
            image_dir: å›¾ç‰‡ç›®å½•
            transform: å›¾åƒå˜æ¢ï¼ˆå¯é€‰ï¼‰
        """
        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {annotation_file}")
        
        with open(annotation_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        self.image_dir = image_dir
        self.transform = transform or self._default_transform()
        
        # éªŒè¯å’Œè¿‡æ»¤æ•°æ®
        valid_data = []
        for item in self.data:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if 'file_name' not in item or 'scene_category' not in item:
                continue
            
            # éªŒè¯åœºæ™¯ç±»åˆ«
            scene = item['scene_category']
            if scene not in SCENE_TO_ID:
                continue
            
            # éªŒè¯å›¾ç‰‡å­˜åœ¨
            image_path = os.path.join(self.image_dir, item['file_name'])
            if not os.path.exists(image_path):
                continue
            
            item['scene_id'] = SCENE_TO_ID[scene]
            valid_data.append(item)
        
        self.data = valid_data
        print(f"âœ… åŠ è½½ {len(self.data)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        
        # ç»Ÿè®¡åœºæ™¯åˆ†å¸ƒ
        self._print_distribution()
    
    def _default_transform(self):
        """é»˜è®¤å›¾åƒå˜æ¢"""
        normalize = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            normalize,
        ])
    
    def _print_distribution(self):
        """æ‰“å°åœºæ™¯åˆ†å¸ƒ"""
        scene_counts = {}
        for item in self.data:
            scene_id = item['scene_id']
            scene_counts[scene_id] = scene_counts.get(scene_id, 0) + 1
        
        print(f"\nğŸ“Š åœºæ™¯åˆ†å¸ƒ:")
        for scene_id in sorted(scene_counts.keys()):
            count = scene_counts[scene_id]
            ratio = count / len(self.data) * 100
            scene_name = ID_TO_SCENE[scene_id]
            print(f"  {scene_name:8s}: {count:4d} ({ratio:5.1f}%)")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # åŠ è½½å›¾åƒ
        image_path = os.path.join(self.image_dir, item['file_name'])
        try:
            image = Image.open(image_path).convert('RGB')
            image = self.transform(image)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å›¾åƒå¤±è´¥: {image_path}, {e}")
            image = torch.zeros(3, 224, 224)
        
        return {
            'image': image,
            'scene_label': item['scene_id'],
            'file_name': item['file_name'],
        }


# ============================================
# å•æ¨¡å‹è¯„ä¼°
# ============================================
@torch.no_grad()
def evaluate_single_model(model, test_loader, device, model_name):
    """
    è¯„ä¼°å•ä¸ªæ¨¡å‹çš„åˆ†ç±»æ€§èƒ½
    
    Args:
        model: æ¨¡å‹åŒ…è£…å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        model_name: æ¨¡å‹åç§°
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸ”¬ è¯„ä¼°æ¨¡å‹: {model_name}")
    print(f"{'='*60}")
    
    all_labels = []
    all_preds = []
    all_probs = []
    
    # é€æ‰¹æ¬¡é¢„æµ‹
    for batch in tqdm(test_loader, desc=f"æ¨ç†ä¸­"):
        images = batch['image'].to(device)
        labels = batch['scene_label']
        
        # è½¬æ¢ä¸º PIL Images
        from torchvision.transforms import ToPILImage
        to_pil = ToPILImage()
        pil_images = [to_pil(img.cpu()) for img in images]
        
        try:
            # è·å–åˆ†ç±» logits
            logits = model.classify_scene(pil_images)
            
            if logits is None:
                raise ValueError(f"{model_name} ä¸æ”¯æŒåœºæ™¯åˆ†ç±»")
            
            # è®¡ç®—é¢„æµ‹
            probs = F.softmax(logits, dim=-1)
            preds = torch.argmax(logits, dim=-1)
            
            all_labels.append(labels)
            all_preds.append(preds.cpu())
            all_probs.append(probs.cpu())
        
        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if len(all_labels) == 0:
        print(f"âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ‰¹æ¬¡")
        return None
    
    # åˆå¹¶ç»“æœ
    all_labels = torch.cat(all_labels, dim=0).numpy()
    all_preds = torch.cat(all_preds, dim=0).numpy()
    all_probs = torch.cat(all_probs, dim=0).numpy()
    
    # ========== è®¡ç®—æŒ‡æ ‡ ==========
    
    # 1. æ€»ä½“å‡†ç¡®ç‡
    accuracy = accuracy_score(all_labels, all_preds) * 100
    
    # 2. æ¯ç±»åˆ«çš„ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1
    precision, recall, f1, support = precision_recall_fscore_support(
        all_labels, all_preds, average=None, zero_division=0
    )
    
    # 3. åŠ æƒå¹³å‡æŒ‡æ ‡
    precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(
        all_labels, all_preds, average='weighted', zero_division=0
    )
    
    # 4. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds)
    
    # 5. Top-5 å‡†ç¡®ç‡
    top5_acc = 0.0
    if NUM_SCENE_CLASSES >= 5:
        top5_preds = np.argsort(all_probs, axis=1)[:, -5:]
        top5_correct = np.array([label in top5_preds[i] for i, label in enumerate(all_labels)])
        top5_acc = top5_correct.mean() * 100
    
    # ========== æ‰“å°ç»“æœ ==========
    print(f"\nğŸ“Š åˆ†ç±»ç»“æœ:")
    print(f"  æ•´ä½“å‡†ç¡®ç‡: {accuracy:.2f}%")
    if NUM_SCENE_CLASSES >= 5:
        print(f"  Top-5 å‡†ç¡®ç‡: {top5_acc:.2f}%")
    print(f"  åŠ æƒç²¾ç¡®ç‡: {precision_avg*100:.2f}%")
    print(f"  åŠ æƒå¬å›ç‡: {recall_avg*100:.2f}%")
    print(f"  åŠ æƒ F1 åˆ†æ•°: {f1_avg*100:.2f}%")
    
    print(f"\nğŸ“‹ å„ç±»åˆ«æŒ‡æ ‡:")
    print(f"{'åœºæ™¯':<10s} {'å‡†ç¡®ç‡':>8s} {'ç²¾ç¡®ç‡':>8s} {'å¬å›ç‡':>8s} {'F1åˆ†æ•°':>8s} {'æ ·æœ¬æ•°':>8s}")
    print("-" * 60)
    
    class_metrics = {}
    for i, scene_name in enumerate(SCENE_CATEGORIES):
        if support[i] > 0:
            class_acc = cm[i, i] / cm[i].sum() * 100 if cm[i].sum() > 0 else 0.0
            class_metrics[scene_name] = {
                'accuracy': float(class_acc),
                'precision': float(precision[i] * 100),
                'recall': float(recall[i] * 100),
                'f1': float(f1[i] * 100),
                'support': int(support[i]),
            }
            print(f"{scene_name:<10s} {class_acc:7.2f}% {precision[i]*100:7.2f}% "
                  f"{recall[i]*100:7.2f}% {f1[i]*100:7.2f}% {support[i]:7d}")
    
    # ========== è¿”å›ç»“æœ ==========
    results = {
        'model_name': model_name,
        'overall': {
            'accuracy': float(accuracy),
            'top5_accuracy': float(top5_acc) if NUM_SCENE_CLASSES >= 5 else None,
            'weighted_precision': float(precision_avg * 100),
            'weighted_recall': float(recall_avg * 100),
            'weighted_f1': float(f1_avg * 100),
            'num_samples': len(all_labels),
        },
        'per_class': class_metrics,
        'confusion_matrix': cm.tolist(),
    }
    
    return results


# ============================================
# å¯è§†åŒ–å‡½æ•°
# ============================================
def plot_accuracy_comparison(all_results: List[Dict], output_path: str):
    """ç»˜åˆ¶å‡†ç¡®ç‡å¯¹æ¯”æŸ±çŠ¶å›¾"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    model_names = [r['model_name'] for r in all_results]
    
    # 1. æ•´ä½“å‡†ç¡®ç‡
    ax = axes[0, 0]
    accuracies = [r['overall']['accuracy'] for r in all_results]
    bars = ax.barh(model_names, accuracies, color='skyblue')
    ax.set_xlabel('Accuracy (%)', fontsize=12)
    ax.set_title('Overall Accuracy Comparison', fontsize=14, fontweight='bold')
    ax.set_xlim([0, 100])
    for bar, acc in zip(bars, accuracies):
        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                f'{acc:.2f}%', va='center', fontsize=10)
    
    # 2. Top-5 å‡†ç¡®ç‡
    ax = axes[0, 1]
    if all_results[0]['overall']['top5_accuracy'] is not None:
        top5_accs = [r['overall']['top5_accuracy'] for r in all_results]
        bars = ax.barh(model_names, top5_accs, color='lightgreen')
        ax.set_xlabel('Top-5 Accuracy (%)', fontsize=12)
        ax.set_title('Top-5 Accuracy Comparison', fontsize=14, fontweight='bold')
        ax.set_xlim([0, 100])
        for bar, acc in zip(bars, top5_accs):
            ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                    f'{acc:.2f}%', va='center', fontsize=10)
    else:
        ax.text(0.5, 0.5, 'Top-5 Accuracy N/A', ha='center', va='center',
                transform=ax.transAxes, fontsize=14)
        ax.axis('off')
    
    # 3. F1 åˆ†æ•°
    ax = axes[1, 0]
    f1_scores = [r['overall']['weighted_f1'] for r in all_results]
    bars = ax.barh(model_names, f1_scores, color='lightcoral')
    ax.set_xlabel('Weighted F1 Score (%)', fontsize=12)
    ax.set_title('Weighted F1 Score Comparison', fontsize=14, fontweight='bold')
    ax.set_xlim([0, 100])
    for bar, f1 in zip(bars, f1_scores):
        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                f'{f1:.2f}%', va='center', fontsize=10)
    
    # 4. å¹³å‡ç±»åˆ«å‡†ç¡®ç‡
    ax = axes[1, 1]
    avg_class_accs = []
    for result in all_results:
        class_accs = [m['accuracy'] for m in result['per_class'].values()]
        avg_class_accs.append(np.mean(class_accs) if class_accs else 0.0)
    
    bars = ax.barh(model_names, avg_class_accs, color='plum')
    ax.set_xlabel('Average Per-Class Accuracy (%)', fontsize=12)
    ax.set_title('Average Per-Class Accuracy', fontsize=14, fontweight='bold')
    ax.set_xlim([0, 100])
    for bar, acc in zip(bars, avg_class_accs):
        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                f'{acc:.2f}%', va='center', fontsize=10)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š å¯¹æ¯”å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def plot_per_class_comparison(all_results: List[Dict], output_path: str):
    """ç»˜åˆ¶å„ç±»åˆ«å‡†ç¡®ç‡å¯¹æ¯”çƒ­åŠ›å›¾"""
    model_names = [r['model_name'] for r in all_results]
    
    # æ„å»ºæ•°æ®çŸ©é˜µ
    data = []
    for result in all_results:
        row = []
        for scene in SCENE_CATEGORIES:
            if scene in result['per_class']:
                row.append(result['per_class'][scene]['accuracy'])
            else:
                row.append(0.0)
        data.append(row)
    
    data = np.array(data)
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    plt.figure(figsize=(14, max(6, len(model_names) * 0.6)))
    sns.heatmap(
        data,
        annot=True,
        fmt='.1f',
        cmap='RdYlGn',
        xticklabels=SCENE_CATEGORIES,
        yticklabels=model_names,
        cbar_kws={'label': 'Accuracy (%)'},
        vmin=0,
        vmax=100,
        linewidths=0.5,
    )
    
    plt.xlabel('Scene Category', fontsize=12)
    plt.ylabel('Model', fontsize=12)
    plt.title('Per-Class Accuracy Comparison', fontsize=14, fontweight='bold', pad=20)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š ç±»åˆ«å¯¹æ¯”çƒ­åŠ›å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def plot_confusion_matrices(all_results: List[Dict], output_dir: str):
    """ä¸ºæ¯ä¸ªæ¨¡å‹ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    for result in all_results:
        model_name = result['model_name']
        cm = np.array(result['confusion_matrix'])
        
        # å½’ä¸€åŒ–
        cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-10)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(
            cm_normalized,
            annot=True,
            fmt='.2f',
            cmap='Blues',
            xticklabels=SCENE_CATEGORIES,
            yticklabels=SCENE_CATEGORIES,
            cbar_kws={'label': 'Normalized Count'},
            vmin=0,
            vmax=1,
        )
        
        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)
        plt.title(f'Confusion Matrix - {model_name}', fontsize=14, fontweight='bold', pad=20)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        safe_name = model_name.replace(' ', '_').replace('/', '-').replace('(', '').replace(')', '')
        output_path = os.path.join(output_dir, f'confusion_matrix_{safe_name}.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ğŸ’¾ {model_name} æ··æ·†çŸ©é˜µå·²ä¿å­˜")


def save_comparison_table(all_results: List[Dict], output_path: str):
    """ä¿å­˜å¯¹æ¯”è¡¨æ ¼ä¸º CSV"""
    rows = []
    for result in all_results:
        row = {
            'Model': result['model_name'],
            'Accuracy (%)': f"{result['overall']['accuracy']:.2f}",
            'Top-5 Acc (%)': f"{result['overall'].get('top5_accuracy', 0):.2f}" if result['overall'].get('top5_accuracy') else 'N/A',
            'Weighted Precision (%)': f"{result['overall']['weighted_precision']:.2f}",
            'Weighted Recall (%)': f"{result['overall']['weighted_recall']:.2f}",
            'Weighted F1 (%)': f"{result['overall']['weighted_f1']:.2f}",
            'Num Samples': result['overall']['num_samples'],
        }
        
        # æ·»åŠ å„ç±»åˆ«å‡†ç¡®ç‡
        for scene in SCENE_CATEGORIES:
            if scene in result['per_class']:
                row[f'{scene} Acc (%)'] = f"{result['per_class'][scene]['accuracy']:.2f}"
            else:
                row[f'{scene} Acc (%)'] = '0.00'
        
        rows.append(row)
    
    df = pd.DataFrame(rows)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ“„ å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜: {output_path}")
    
    # æ‰“å°è¡¨æ ¼é¢„è§ˆ
    print(f"\nğŸ“Š å¯¹æ¯”è¡¨æ ¼é¢„è§ˆ:")
    print(df[['Model', 'Accuracy (%)', 'Weighted F1 (%)']].to_string(index=False))


# ============================================
# ä¸»å‡½æ•°
# ============================================
def main():
    """ä¸»å‡½æ•° - æ‰¹é‡æµ‹è¯•å¤šä¸ªæ¨¡å‹"""
    
    config = {
        # æ•°æ®
        'test_file': '/workspace/vlm/lab/output/test_split.json',
        'image_dir': '/data/fasion/train/image',
        'batch_size': 32,
        'num_workers': 4,
        
        # è®¾å¤‡
        'device': 'cuda:4',
        
        # è¾“å‡º
        'output_dir': '/workspace/vlm/lab/output/classification_comparison',
        
        # é›¶æ ·æœ¬é…ç½®
        'zero_shot_template': 'descriptive',  # 'simple', 'descriptive', 'context'
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    print("="*60)
    print("ğŸ”¬ åœºæ™¯åˆ†ç±»å¯¹æ¯”å®éªŒ")
    print("="*60)
    
    # ç”Ÿæˆåœºæ™¯æè¿°ï¼ˆç”¨äºé›¶æ ·æœ¬åˆ†ç±»ï¼‰
    scene_prompts = generate_scene_prompts(config['zero_shot_template'])
    
    # ========== å®šä¹‰è¦å¯¹æ¯”çš„æ¨¡å‹ ==========
    models_to_compare = [
        # ========== 1. é›¶æ ·æœ¬ CLIP ç³»åˆ— ==========
        {
            'name': 'CLIP-ViT-B/32 (Zero-shot)',
            'model_name': 'clip-vit-b32',
            'kwargs': {'num_classes': 10},
            'use_zero_shot': True,
        },
        {
            'name': 'CLIP-ViT-L/14 (Zero-shot)',
            'model_name': 'clip-vit-l14',
            'kwargs': {'num_classes': 10},
            'use_zero_shot': True,
        },
        
        # ========== 2. å¾®è°ƒåçš„ CLIP ç³»åˆ— ==========
        {
            'name': 'CLIP-ViT-B/32 (Finetuned)',
            'model_name': 'clip-vit-b32',
            'kwargs': {
                'checkpoint_path': 'outputs/finetuned_clip-vit-b32/clip-vit-b32_best.pth',
                'num_classes': 10,
            },
            'use_zero_shot': False,
        },
        {
            'name': 'CLIP-ViT-L/14 (Finetuned)',
            'model_name': 'clip-vit-l14',
            'kwargs': {
                'checkpoint_path': 'outputs/finetuned_clip-vit-l14/clip-vit-l14_best.pth',
                'num_classes': 10,
            },
            'use_zero_shot': False,
        },
        
        # ========== 3. ResNet-50 ==========
        {
            'name': 'ResNet-50 (Finetuned)',
            'model_name': 'resnet50',
            'kwargs': {
                'checkpoint_path': 'outputs/finetuned_resnet50/resnet50_best.pth',
                'num_classes': 10,
            },
            'use_zero_shot': False,
        },
        
        # ========== 4. ViT-Base ==========
        {
            'name': 'ViT-Base (Finetuned)',
            'model_name': 'vit-base',
            'kwargs': {
                'checkpoint_path': 'outputs/finetuned_vit-base/vit-base_best.pth',
                'num_classes': 10,
            },
            'use_zero_shot': False,
        },
        
        # ========== 5. BLIP2 + LoRA ==========
        {
            'name': 'BLIP2-LoRA (Ours)',
            'model_name': 'blip2-lora',
            'kwargs': {
                'base_checkpoint': 'checkpoint_04.pth',
                'lora_checkpoint': 'outputs/fashion_lora_itc_scene/best_model',
                'scene_head_path': 'outputs/fashion_lora_itc_scene/best_model/scene_head.pth',
            },
            'use_zero_shot': False,
        },
    ]
    
    # ========== åŠ è½½æµ‹è¯•æ•°æ® ==========
    test_dataset = ClassificationTestDataset(
        config['test_file'],
        config['image_dir']
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=True,
        collate_fn=lambda x: {
            'image': torch.stack([item['image'] for item in x]),
            'scene_label': torch.tensor([item['scene_label'] for item in x]),
        }
    )
    
    # ========== è¯„ä¼°æ‰€æœ‰æ¨¡å‹ ==========
    all_results = []
    
    for model_config in models_to_compare:
        try:
            print(f"\n{'#'*60}")
            print(f"# {model_config['name']}")
            print(f"{'#'*60}")
            
            # åˆ›å»ºåŸºç¡€æ¨¡å‹
            base_model = create_model(
                model_config['model_name'],
                device=config['device'],
                **model_config['kwargs']
            )
            
            # å¦‚æœéœ€è¦é›¶æ ·æœ¬åˆ†ç±»ï¼ŒåŒ…è£…æ¨¡å‹
            if model_config.get('use_zero_shot', False):
                model = ExtendedModelWrapper(
                    base_model,
                    use_zero_shot=True,
                    scene_prompts=scene_prompts
                )
            else:
                model = base_model
            
            # è¯„ä¼°
            results = evaluate_single_model(
                model,
                test_loader,
                config['device'],
                model_config['name']
            )
            
            if results is not None:
                all_results.append(results)
                
                # ä¿å­˜å•ä¸ªæ¨¡å‹ç»“æœ
                safe_name = model_config['name'].replace(' ', '_').replace('/', '-').replace('(', '').replace(')', '')
                result_file = os.path.join(config['output_dir'], f"{safe_name}_results.json")
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
            
            # é‡Šæ”¾å†…å­˜
            del model
            if 'base_model' in locals():
                del base_model
            torch.cuda.empty_cache()
        
        except Exception as e:
            print(f"âŒ è¯„ä¼° {model_config['name']} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ========== ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š ==========
    if len(all_results) == 0:
        print("âŒ æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„æ¨¡å‹")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
    print(f"{'='*60}")
    
    # 1. ä¿å­˜æ±‡æ€» JSON
    summary_file = os.path.join(config['output_dir'], 'comparison_summary.json')
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… æ±‡æ€»ç»“æœå·²ä¿å­˜: {summary_file}")
    
    # 2. ä¿å­˜å¯¹æ¯”è¡¨æ ¼
    table_file = os.path.join(config['output_dir'], 'comparison_table.csv')
    save_comparison_table(all_results, table_file)
    
    # 3. ç»˜åˆ¶å¯¹æ¯”å›¾
    comparison_plot = os.path.join(config['output_dir'], 'accuracy_comparison.png')
    plot_accuracy_comparison(all_results, comparison_plot)
    
    # 4. ç»˜åˆ¶ç±»åˆ«å¯¹æ¯”çƒ­åŠ›å›¾
    heatmap_plot = os.path.join(config['output_dir'], 'per_class_comparison.png')
    plot_per_class_comparison(all_results, heatmap_plot)
    
    # 5. ç»˜åˆ¶æ‰€æœ‰æ··æ·†çŸ©é˜µ
    print(f"\nğŸ“Š ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
    plot_confusion_matrices(all_results, config['output_dir'])
    
    # ========== æœ€ç»ˆæ€»ç»“ ==========
    print(f"\n{'='*60}")
    print(f"âœ… å¯¹æ¯”å®éªŒå®Œæˆï¼")
    print(f"{'='*60}")
    
    print(f"\nğŸ† æ’å (æŒ‰å‡†ç¡®ç‡):")
    sorted_results = sorted(all_results, key=lambda x: x['overall']['accuracy'], reverse=True)
    for i, result in enumerate(sorted_results, 1):
        print(f"  {i}. {result['model_name']:<35s}: {result['overall']['accuracy']:.2f}%")
    
    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {config['output_dir']}")


if __name__ == "__main__":
    main()