#!/usr/bin/env python3
"""
BLIP2 + LoRA å›¾æ–‡æ£€ç´¢ + åœºæ™¯åˆ†ç±» ç»¼åˆæµ‹è¯•è„šæœ¬
ä½¿ç”¨åœºæ™¯ç±»åˆ«åç§°ä½œä¸ºæ–‡æœ¬è¿›è¡Œæ£€ç´¢å’Œåˆ†ç±»è¯„ä¼°
"""

import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from torchvision import transforms
from tqdm import tqdm
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from peft import PeftModel
from models.blip2_qformer import Blip2Qformer


# ============================================
# åœºæ™¯ç±»åˆ«å®šä¹‰ï¼ˆä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
# ============================================
SCENE_CATEGORIES = [
    'èŒåœºæ­£è£…', 'èŒåœºä¼‘é—²', 'è¿åŠ¨å¥èº«', 'æˆ·å¤–æ¢é™©', 'å±…å®¶ä¼‘é—²',
    'ç¤¾äº¤èšä¼š', 'æ—…è¡Œåº¦å‡', 'è¿åŠ¨èµ›äº‹', 'å©šç¤¼ç›¸å…³', 'ç‰¹æ®ŠåŠŸèƒ½',
]
SCENE_TO_ID = {name: idx for idx, name in enumerate(SCENE_CATEGORIES)}
ID_TO_SCENE = {idx: name for idx, name in enumerate(SCENE_CATEGORIES)}
NUM_SCENE_CLASSES = len(SCENE_CATEGORIES)


# ============================================
# åœºæ™¯åˆ†ç±»å¤´ï¼ˆä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
# ============================================
class SceneClassificationHead(nn.Module):
    """åœºæ™¯åˆ†ç±»å¤´"""
    def __init__(self, input_dim=256, num_classes=10, dropout=0.1):
        super().__init__()
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, input_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(input_dim, num_classes)
        )
    
    def forward(self, x):
        return self.classifier(x)


# ============================================
# æµ‹è¯•æ•°æ®é›†ï¼ˆä½¿ç”¨åœºæ™¯ç±»åˆ«åç§°ä½œä¸ºæ–‡æœ¬ï¼‰
# ============================================
class TestDataset(Dataset):
    """å›¾æ–‡æ£€ç´¢ + åœºæ™¯åˆ†ç±»æµ‹è¯•æ•°æ®é›† - ä½¿ç”¨ text + "ï¼Œé€‚åˆ" + scene_category"""
    def __init__(self, annotation_file, image_dir, transform=None, use_key_features=False):
        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {annotation_file}")
        
        with open(annotation_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        self.image_dir = image_dir
        self.transform = transform or self._default_transform()
        self.use_key_features = use_key_features
        
        # éªŒè¯æ•°æ®
        valid_data = []
        for item in self.data:
            if 'file_name' not in item or 'scene_category' not in item:
                continue
            
            # éªŒè¯åœºæ™¯æ ‡ç­¾
            scene = item['scene_category']
            if scene not in SCENE_TO_ID:
                continue
            
            # éªŒè¯å›¾ç‰‡å­˜åœ¨
            image_path = os.path.join(self.image_dir, item['file_name'])
            if not os.path.exists(image_path):
                continue
            
            item['scene_id'] = SCENE_TO_ID[scene]
            
            # ===== ä¿®æ”¹ï¼šæ‹¼æ¥ text å’Œ scene_category =====
            if self.use_key_features and 'key_features' in item:
                # ä½¿ç”¨ key_features
                item['combined_text'] = item['key_features']
            else:
                # æ‹¼æ¥ text + "ï¼Œé€‚åˆ" + scene_category
                text_content = item.get('text', '')
                scene_category = item.get('scene_category', '')
                
                if text_content and scene_category:
                    item['combined_text'] = f"{text_content}ï¼Œé€‚åˆ{scene_category}"
                elif text_content:
                    item['combined_text'] = text_content
                else:
                    item['combined_text'] = scene_category
            
            valid_data.append(item)
        
        self.data = valid_data
        print(f"âœ… åŠ è½½ {len(self.data)} ä¸ªæœ‰æ•ˆæµ‹è¯•æ ·æœ¬")
        print(f"ğŸ’¡ ä½¿ç”¨æ–‡æœ¬æ ¼å¼: text + \"ï¼Œé€‚åˆ\" + scene_category")
        
        # æ‰“å°å‡ ä¸ªç¤ºä¾‹
        print(f"\nğŸ“ æ–‡æœ¬ç¤ºä¾‹:")
        for i in range(min(3, len(self.data))):
            print(f"  æ ·æœ¬ {i+1}: {self.data[i]['combined_text'][:100]}...")
        
        # ç»Ÿè®¡åœºæ™¯åˆ†å¸ƒ
        scene_counts = {}
        for item in self.data:
            scene_id = item['scene_id']
            scene_counts[scene_id] = scene_counts.get(scene_id, 0) + 1
        
        print(f"\nğŸ“Š åœºæ™¯åˆ†å¸ƒ:")
        for scene_id in sorted(scene_counts.keys()):
            count = scene_counts[scene_id]
            ratio = count / len(self.data) * 100
            print(f"  {ID_TO_SCENE[scene_id]:8s}: {count:4d} ({ratio:5.1f}%)")
    
    def _default_transform(self):
        normalize = transforms.Normalize(
            mean=[0.48145466, 0.4578275, 0.40821073],
            std=[0.26862954, 0.26130258, 0.27577711]
        )
        return transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            normalize,
        ])
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # åŠ è½½å›¾åƒ
        image_path = os.path.join(self.image_dir, item['file_name'])
        try:
            image = Image.open(image_path).convert('RGB')
            image = self.transform(image)
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å›¾åƒå¤±è´¥: {image_path}, é”™è¯¯: {e}")
            image = torch.zeros(3, 224, 224)
        
        return {
            'image': image,
            'text': item['combined_text'],  # ä½¿ç”¨æ‹¼æ¥åçš„æ–‡æœ¬
            'scene_label': item['scene_id'],
            'file_name': item['file_name'],
            'idx': idx,
        }

# ============================================
# æ¨¡å‹åŠ è½½
# ============================================
def load_model_with_lora(base_checkpoint, lora_checkpoint, scene_head_path, device):
    """
    åŠ è½½åŸºç¡€æ¨¡å‹ + LoRA + åœºæ™¯åˆ†ç±»å¤´
    
    Args:
        base_checkpoint: åŸºç¡€ BLIP2 æƒé‡è·¯å¾„
        lora_checkpoint: LoRA é€‚é…å™¨æƒé‡ç›®å½•
        scene_head_path: åœºæ™¯åˆ†ç±»å¤´æƒé‡è·¯å¾„
        device: è®¾å¤‡
    
    Returns:
        model: åŠ è½½äº† LoRA çš„æ¨¡å‹
        scene_head: åœºæ™¯åˆ†ç±»å¤´
    """
    print(f"\nğŸ“¥ åŠ è½½æ¨¡å‹...")
    print(f"  åŸºç¡€æƒé‡: {base_checkpoint}")
    print(f"  LoRA æƒé‡: {lora_checkpoint}")
    print(f"  åˆ†ç±»å¤´æƒé‡: {scene_head_path}")
    
    # 1. åŠ è½½åŸºç¡€æ¨¡å‹
    model = Blip2Qformer(
        vit_model="clip_L",
        img_size=224,
        freeze_vit=True,
        num_query_token=32,
        embed_dim=256,
        max_txt_len=77,
    )
    
    if os.path.exists(base_checkpoint):
        checkpoint = torch.load(base_checkpoint, map_location='cpu')
        state_dict = checkpoint.get("model", checkpoint)
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        print(f"âœ… åŠ è½½åŸºç¡€æƒé‡")
    else:
        print(f"âš ï¸ æœªæ‰¾åˆ°åŸºç¡€æƒé‡: {base_checkpoint}")
    
    # 2. åŠ è½½ LoRA é€‚é…å™¨
    if os.path.exists(lora_checkpoint):
        print(f"ğŸ”§ åŠ è½½ LoRA é€‚é…å™¨...")
        model.Qformer = PeftModel.from_pretrained(
            model.Qformer,
            lora_checkpoint,
            is_trainable=False
        )
        print(f"âœ… LoRA æƒé‡åŠ è½½æˆåŠŸ")
    else:
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ° LoRA æƒé‡: {lora_checkpoint}")
    
    model.to(device)
    model.eval()
    
    # 3. åŠ è½½åœºæ™¯åˆ†ç±»å¤´
    scene_head = SceneClassificationHead(
        input_dim=256,
        num_classes=NUM_SCENE_CLASSES,
        dropout=0.1
    ).to(device)
    
    if os.path.exists(scene_head_path):
        scene_head.load_state_dict(torch.load(scene_head_path, map_location=device))
        print(f"âœ… åœºæ™¯åˆ†ç±»å¤´åŠ è½½æˆåŠŸ")
    else:
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°åˆ†ç±»å¤´æƒé‡: {scene_head_path}")
    
    scene_head.eval()
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    print(f"\nğŸ’¾ æ¨¡å‹å‚æ•°: {total_params:,}")
    
    return model, scene_head


# ============================================
# ç‰¹å¾æå– + åˆ†ç±»é¢„æµ‹
# ============================================
@torch.no_grad()
def extract_features_and_predict(model, scene_head, dataloader, device):
    """
    æå–å›¾åƒ/æ–‡æœ¬ç‰¹å¾ + åœºæ™¯åˆ†ç±»é¢„æµ‹
    
    Returns:
        image_feats: [N, embed_dim]
        text_feats: [N, embed_dim]
        scene_labels: [N] çœŸå®æ ‡ç­¾
        scene_preds: [N] é¢„æµ‹æ ‡ç­¾
        scene_probs: [N, num_classes] é¢„æµ‹æ¦‚ç‡
        indices: [N] æ ·æœ¬ç´¢å¼•
    """
    print(f"\nğŸ” æå–ç‰¹å¾å¹¶é¢„æµ‹...")
    
    image_feats_list = []
    text_feats_list = []
    scene_labels_list = []
    scene_preds_list = []
    scene_probs_list = []
    indices_list = []
    
    for batch in tqdm(dataloader, desc="å¤„ç†æ‰¹æ¬¡"):
        images = batch['image'].to(device)
        texts = batch['text']
        scene_labels = batch['scene_label']
        indices = batch['idx']
        
        # æå–ç‰¹å¾
        image_feats, text_feats = model({'image': images, 'text': texts})
        
        # å¤„ç† image_feats ç»´åº¦
        if image_feats.dim() == 3:
            image_feats = image_feats.mean(dim=1)
        
        # åœºæ™¯åˆ†ç±»é¢„æµ‹
        logits = scene_head(image_feats)
        probs = F.softmax(logits, dim=-1)
        preds = logits.argmax(dim=-1)
        
        # L2 å½’ä¸€åŒ–ï¼ˆç”¨äºæ£€ç´¢ï¼‰
        image_feats_norm = F.normalize(image_feats, dim=-1)
        text_feats_norm = F.normalize(text_feats, dim=-1)
        
        # æ”¶é›†ç»“æœ
        image_feats_list.append(image_feats_norm.cpu())
        text_feats_list.append(text_feats_norm.cpu())
        scene_labels_list.append(scene_labels)
        scene_preds_list.append(preds.cpu())
        scene_probs_list.append(probs.cpu())
        indices_list.extend(indices.tolist())
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    image_feats = torch.cat(image_feats_list, dim=0)
    text_feats = torch.cat(text_feats_list, dim=0)
    scene_labels = torch.cat(scene_labels_list, dim=0)
    scene_preds = torch.cat(scene_preds_list, dim=0)
    scene_probs = torch.cat(scene_probs_list, dim=0)
    
    print(f"âœ… å¤„ç†å®Œæˆ")
    print(f"  å›¾åƒç‰¹å¾: {image_feats.shape}")
    print(f"  æ–‡æœ¬ç‰¹å¾: {text_feats.shape}")
    print(f"  åœºæ™¯æ ‡ç­¾: {scene_labels.shape}")
    
    return image_feats, text_feats, scene_labels, scene_preds, scene_probs, indices_list


# ============================================
# å›¾æ–‡æ£€ç´¢è¯„ä¼°ï¼ˆåœºæ™¯çº§åˆ«ï¼‰
# ============================================
def evaluate_scene_based_retrieval(image_feats, text_feats, scene_labels, top_k=[1, 5, 10]):
    """
    åŸºäºåœºæ™¯çš„å›¾æ–‡æ£€ç´¢è¯„ä¼°
    ç”±äºæ¯ä¸ªåœºæ™¯ç±»åˆ«æœ‰å¤šä¸ªæ ·æœ¬ï¼Œè¯„ä¼°æ—¶è€ƒè™‘åŒç±»åˆ«æ ·æœ¬çš„æ£€ç´¢æ€§èƒ½
    æ–‡æœ¬æ ¼å¼: text + "ï¼Œé€‚åˆ" + scene_category
    """
    print(f"\n" + "="*60)
    print(f"ğŸ“Š åŸºäºåœºæ™¯çš„å›¾æ–‡æ£€ç´¢è¯„ä¼°")
    print(f"ğŸ’¡ è¯„ä¼°æŒ‡æ ‡: èƒ½å¦æ£€ç´¢åˆ°ç›¸åŒåœºæ™¯ç±»åˆ«çš„æ ·æœ¬")
    print(f"ğŸ’¡ æ–‡æœ¬æ ¼å¼: text + \"ï¼Œé€‚åˆ\" + scene_category")
    print(f"="*60)
    
    N = image_feats.size(0)
    sim_matrix = image_feats @ text_feats.t()
    
    metrics = {}
    
    # ========== Image-to-Text (åœºæ™¯çº§åˆ«) ==========
    print(f"\nğŸ“·â¡ï¸ğŸ“ å›¾åƒæ£€ç´¢æ–‡æœ¬ (åŒåœºæ™¯è§†ä¸ºæ­£ç¡®):")
    ranks = []
    for i in range(N):
        query_scene = scene_labels[i].item()
        sims = sim_matrix[i]
        sorted_indices = torch.argsort(sims, descending=True)
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒåœºæ™¯æ ·æœ¬çš„æ’å
        for rank, idx in enumerate(sorted_indices, 1):
            if scene_labels[idx].item() == query_scene:
                ranks.append(rank)
                break
    
    ranks = np.array(ranks)
    for k in top_k:
        recall = (ranks <= k).mean() * 100
        metrics[f'i2t_R@{k}'] = recall
        print(f"  R@{k:2d}: {recall:6.2f}%")
    
    metrics['i2t_median_rank'] = float(np.median(ranks))
    metrics['i2t_mean_rank'] = float(np.mean(ranks))
    print(f"  Median Rank: {metrics['i2t_median_rank']:.1f}")
    print(f"  Mean Rank: {metrics['i2t_mean_rank']:.1f}")
    
    # ========== Text-to-Image (åœºæ™¯çº§åˆ«) ==========
    print(f"\nğŸ“â¡ï¸ğŸ“· æ–‡æœ¬æ£€ç´¢å›¾åƒ (åŒåœºæ™¯è§†ä¸ºæ­£ç¡®):")
    sim_matrix_t = sim_matrix.t()
    ranks = []
    for i in range(N):
        query_scene = scene_labels[i].item()
        sims = sim_matrix_t[i]
        sorted_indices = torch.argsort(sims, descending=True)
        
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒåœºæ™¯æ ·æœ¬çš„æ’å
        for rank, idx in enumerate(sorted_indices, 1):
            if scene_labels[idx].item() == query_scene:
                ranks.append(rank)
                break
    
    ranks = np.array(ranks)
    for k in top_k:
        recall = (ranks <= k).mean() * 100
        metrics[f't2i_R@{k}'] = recall
        print(f"  R@{k:2d}: {recall:6.2f}%")
    
    metrics['t2i_median_rank'] = float(np.median(ranks))
    metrics['t2i_mean_rank'] = float(np.mean(ranks))
    print(f"  Median Rank: {metrics['t2i_median_rank']:.1f}")
    print(f"  Mean Rank: {metrics['t2i_mean_rank']:.1f}")
    
    # ========== å¹³å‡æŒ‡æ ‡ ==========
    print(f"\nğŸ“ˆ å¹³å‡æ£€ç´¢æ€§èƒ½:")
    for k in top_k:
        avg_recall = (metrics[f'i2t_R@{k}'] + metrics[f't2i_R@{k}']) / 2
        metrics[f'avg_R@{k}'] = avg_recall
        print(f"  Avg R@{k:2d}: {avg_recall:6.2f}%")
    
    return metrics


# ============================================
# åœºæ™¯åˆ†ç±»è¯„ä¼°
# ============================================
def evaluate_classification(scene_labels, scene_preds, scene_probs):
    """è®¡ç®—åœºæ™¯åˆ†ç±»æŒ‡æ ‡"""
    print(f"\n" + "="*60)
    print(f"ğŸ¯ åœºæ™¯åˆ†ç±»è¯„ä¼°")
    print(f"="*60)
    
    scene_labels = scene_labels.numpy()
    scene_preds = scene_preds.numpy()
    
    # æ€»ä½“å‡†ç¡®ç‡
    accuracy = (scene_labels == scene_preds).mean() * 100
    print(f"\næ•´ä½“å‡†ç¡®ç‡: {accuracy:.2f}%")
    
    # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
    print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    report = classification_report(
        scene_labels,
        scene_preds,
        target_names=SCENE_CATEGORIES,
        digits=4,
        zero_division=0
    )
    print(report)
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(scene_labels, scene_preds)
    
    metrics = {
        'accuracy': float(accuracy),
        'confusion_matrix': cm.tolist(),
    }
    
    # æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    print(f"\nğŸ“Š å„åœºæ™¯å‡†ç¡®ç‡:")
    class_acc = {}
    for i, scene_name in enumerate(SCENE_CATEGORIES):
        if cm[i].sum() > 0:
            acc = cm[i, i] / cm[i].sum() * 100
            class_acc[scene_name] = float(acc)
            print(f"  {scene_name:8s}: {acc:6.2f}% ({cm[i, i]}/{cm[i].sum()})")
        else:
            class_acc[scene_name] = 0.0
    
    metrics['class_accuracy'] = class_acc
    
    return metrics, cm


# ============================================
# å¯è§†åŒ–æ··æ·†çŸ©é˜µ
# ============================================
def plot_confusion_matrix(cm, output_path):
    """ç»˜åˆ¶å¹¶ä¿å­˜æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(12, 10))
    
    # å½’ä¸€åŒ–åˆ° [0, 1]
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    sns.heatmap(
        cm_normalized,
        annot=True,
        fmt='.2f',
        cmap='Blues',
        xticklabels=SCENE_CATEGORIES,
        yticklabels=SCENE_CATEGORIES,
        cbar_kws={'label': 'Normalized Count'}
    )
    
    plt.xlabel('Predicted Label', fontsize=12)
    plt.ylabel('True Label', fontsize=12)
    plt.title('Scene Classification Confusion Matrix', fontsize=14, pad=20)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ’¾ æ··æ·†çŸ©é˜µå·²ä¿å­˜: {output_path}")
    plt.close()


# ============================================
# ä¿å­˜ç»“æœ
# ============================================
def save_results(retrieval_metrics, classification_metrics, output_file):
    """ä¿å­˜æ‰€æœ‰è¯„ä¼°ç»“æœ"""
    results = {
        'retrieval': retrieval_metrics,
        'classification': classification_metrics,
        'text_format': 'text + "ï¼Œé€‚åˆ" + scene_category',
        'note': 'ä½¿ç”¨å®Œæ•´æè¿°æ–‡æœ¬+åœºæ™¯ç±»åˆ«è¿›è¡Œæ£€ç´¢è¯„ä¼°'
    }
    
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜: {output_file}")


# ============================================
# ä¸»å‡½æ•°
# ============================================
def main():
    config = {
        # æ•°æ®
        'test_file': '/workspace/vlm/lab/output/test_split.json',
        'image_dir': '/data/fasion/train/image',
        'use_key_features': False,  # æ˜¯å¦ä½¿ç”¨ key_featuresï¼ˆFalse åˆ™ä½¿ç”¨ text + scene_categoryï¼‰
        
        # æ¨¡å‹
        'base_checkpoint': 'checkpoint_04.pth',
        'lora_checkpoint': 'outputs/fashion_lora_itc_scene/best_model',
        'scene_head_path': 'outputs/fashion_lora_itc_scene/best_model/scene_head.pth',
        
        # è¯„ä¼°
        'batch_size': 64,
        'num_workers': 8,
        'top_k': [1, 5, 10, 20],
        
        # è¾“å‡º
        'output_dir': '/workspace/vlm/lab/output',
        'results_file': '/workspace/vlm/lab/output/test_results_text_scene.json',
        'confusion_matrix_file': '/workspace/vlm/lab/output/confusion_matrix_text_scene.png',
        
        'device': 'cuda:4',
    }
    
    print("="*60)
    print("ğŸ”¬ BLIP2 + LoRA ç»¼åˆæµ‹è¯•")
    print("  - å›¾æ–‡æ£€ç´¢ (text + \"ï¼Œé€‚åˆ\" + scene_category)")
    print("  - åœºæ™¯åˆ†ç±» (Scene Classification)")
    print("="*60)
    
    # æ£€æŸ¥æ–‡ä»¶
    for key in ['test_file', 'image_dir', 'base_checkpoint', 'lora_checkpoint', 'scene_head_path']:
        path = config[key]
        if not os.path.exists(path):
            raise FileNotFoundError(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}")
    
    # åŠ è½½æ•°æ®é›†
    test_dataset = TestDataset(
        config['test_file'], 
        config['image_dir'],
        use_key_features=config['use_key_features']
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        pin_memory=True,
        collate_fn=lambda x: {
            'image': torch.stack([item['image'] for item in x]),
            'text': [item['text'] for item in x],
            'scene_label': torch.tensor([item['scene_label'] for item in x]),
            'idx': torch.tensor([item['idx'] for item in x]),
        }
    )
    
    # åŠ è½½æ¨¡å‹
    model, scene_head = load_model_with_lora(
        config['base_checkpoint'],
        config['lora_checkpoint'],
        config['scene_head_path'],
        config['device']
    )
    
    # æå–ç‰¹å¾ + é¢„æµ‹
    image_feats, text_feats, scene_labels, scene_preds, scene_probs, indices = \
        extract_features_and_predict(model, scene_head, test_loader, config['device'])
    
    # ========== è¯„ä¼°æ£€ç´¢ï¼ˆåœºæ™¯çº§åˆ«ï¼‰ ==========
    retrieval_metrics = evaluate_scene_based_retrieval(
        image_feats, text_feats, scene_labels, top_k=config['top_k']
    )
    
    # ========== è¯„ä¼°åˆ†ç±» ==========
    classification_metrics, cm = evaluate_classification(
        scene_labels, scene_preds, scene_probs
    )
    
    # ========== å¯è§†åŒ–æ··æ·†çŸ©é˜µ ==========
    plot_confusion_matrix(cm, config['confusion_matrix_file'])
    
    # ========== ä¿å­˜ç»“æœ ==========
    save_results(retrieval_metrics, classification_metrics, config['results_file'])
    
    # ========== æ€»ç»“ ==========
    print("\n" + "="*60)
    print("âœ… æµ‹è¯•å®Œæˆ!")
    print("="*60)
    print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡:")
    print(f"  åœºæ™¯æ£€ç´¢ Avg R@1:  {retrieval_metrics['avg_R@1']:.2f}%")
    print(f"  åœºæ™¯æ£€ç´¢ Avg R@5:  {retrieval_metrics['avg_R@5']:.2f}%")
    print(f"  åœºæ™¯åˆ†ç±»å‡†ç¡®ç‡:    {classification_metrics['accuracy']:.2f}%")
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"  ç»“æœJSON: {config['results_file']}")
    print(f"  æ··æ·†çŸ©é˜µ: {config['confusion_matrix_file']}")


if __name__ == "__main__":
    main()

