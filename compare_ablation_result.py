#!/usr/bin/env python3
"""
å¯¹æ¯”æ‰€æœ‰æ¶ˆèå®éªŒçš„åœºæ™¯åˆ†ç±»å‡†ç¡®ç‡
åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¯ä¸ªæ¶ˆèå®éªŒæ¨¡å‹çš„æ€§èƒ½
"""

import os
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from PIL import Image
from tqdm import tqdm
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

# å¯¼å…¥æ¨¡å‹
from models.model_zoo import create_model

# ============================================
# åœºæ™¯ç±»åˆ«å®šä¹‰
# ============================================
SCENE_CATEGORIES = [
    'èŒåœºæ­£è£…', 'èŒåœºä¼‘é—²', 'è¿åŠ¨å¥èº«', 'æˆ·å¤–æ¢é™©', 'å±…å®¶ä¼‘é—²',
    'ç¤¾äº¤èšä¼š', 'æ—…è¡Œåº¦å‡', 'è¿åŠ¨èµ›äº‹', 'å©šç¤¼ç›¸å…³', 'ç‰¹æ®ŠåŠŸèƒ½',
]
SCENE_TO_ID = {name: idx for idx, name in enumerate(SCENE_CATEGORIES)}
ID_TO_SCENE = {idx: name for idx, name in enumerate(SCENE_CATEGORIES)}
NUM_SCENE_CLASSES = len(SCENE_CATEGORIES)


# ============================================
# é›¶æ ·æœ¬åˆ†ç±»å™¨ï¼ˆç”¨äºæ²¡æœ‰åœºæ™¯åˆ†ç±»å¤´çš„æ¨¡å‹ï¼‰
# ============================================
class ZeroShotSceneClassifier:
    """
    é›¶æ ·æœ¬åœºæ™¯åˆ†ç±»å™¨
    é€šè¿‡è®¡ç®—å›¾åƒç‰¹å¾å’Œåœºæ™¯æ–‡æœ¬æè¿°çš„ç›¸ä¼¼åº¦è¿›è¡Œåˆ†ç±»
    """
    def __init__(self, model, device='cuda'):
        """
        Args:
            model: BLIP2 æ¨¡å‹åŒ…è£…å™¨
            device: è®¾å¤‡
        """
        self.model = model
        self.device = device
        
        # ç”Ÿæˆåœºæ™¯æè¿°æ–‡æœ¬
        self.scene_prompts = self._generate_scene_prompts()
        
        print(f"  ğŸ”„ é¢„è®¡ç®—åœºæ™¯æ–‡æœ¬ç‰¹å¾...")
        
        # åˆ›å»º dummy å›¾åƒ
        dummy_images = [Image.new('RGB', (224, 224), color=(128, 128, 128))] * len(self.scene_prompts)
        
        try:
            # æå–åœºæ™¯æ–‡æœ¬ç‰¹å¾
            _, text_feats = model.extract_features(dummy_images, self.scene_prompts)
            
            if text_feats is None:
                raise ValueError("æ¨¡å‹ä¸æ”¯æŒæ–‡æœ¬ç‰¹å¾æå–")
            
            # å½’ä¸€åŒ–æ–‡æœ¬ç‰¹å¾
            self.scene_text_feats = F.normalize(text_feats, dim=-1)
            print(f"  âœ… åœºæ™¯æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {self.scene_text_feats.shape}")
        
        except Exception as e:
            print(f"  âŒ æ–‡æœ¬ç‰¹å¾æå–å¤±è´¥: {e}")
            raise
    
    def _generate_scene_prompts(self):
        """ç”Ÿæˆåœºæ™¯æè¿°æ–‡æœ¬"""
        prompts = [
            'é€‚åˆèŒåœºæ­£å¼åœºåˆçš„æ­£è£…æœé¥°',
            'é€‚åˆèŒåœºçš„å•†åŠ¡ä¼‘é—²æœè£…',
            'é€‚åˆè¿åŠ¨å¥èº«çš„è¿åŠ¨æœè£…',
            'é€‚åˆæˆ·å¤–æ¢é™©çš„åŠŸèƒ½æ€§æœè£…',
            'é€‚åˆå®¶ä¸­ç©¿ç€çš„å±…å®¶ä¼‘é—²æœ',
            'é€‚åˆç¤¾äº¤èšä¼šçš„æ—¶å°šæœè£…',
            'é€‚åˆæ—…è¡Œåº¦å‡çš„è½»ä¾¿æœè£…',
            'é€‚åˆè¿åŠ¨èµ›äº‹çš„ä¸“ä¸šè£…å¤‡',
            'é€‚åˆå©šç¤¼åœºåˆçš„ç¤¼æœ',
            'å…·æœ‰ç‰¹æ®ŠåŠŸèƒ½çš„æœè£…',
        ]
        print(f"  ğŸ“ åœºæ™¯æè¿°ç¤ºä¾‹: {prompts[0]}")
        return prompts
    
    @torch.no_grad()
    def predict(self, images):
        """
        é›¶æ ·æœ¬é¢„æµ‹
        
        Args:
            images: PIL Images åˆ—è¡¨
        
        Returns:
            logits: [B, num_classes]
        """
        # æå–å›¾åƒç‰¹å¾
        image_feats, _ = self.model.extract_features(images, [""] * len(images))
        
        # å½’ä¸€åŒ–å›¾åƒç‰¹å¾
        image_feats = F.normalize(image_feats, dim=-1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦ä½œä¸º logits
        logits = image_feats @ self.scene_text_feats.t()
        
        # ç¼©æ”¾åˆ°æ›´åˆç†çš„èŒƒå›´
        logits = logits * 100.0
        
        return logits


# ============================================
# æ‰©å±•çš„æ¨¡å‹åŒ…è£…å™¨ï¼ˆæ”¯æŒé›¶æ ·æœ¬åˆ†ç±»ï¼‰
# ============================================
class ExtendedBLIP2Wrapper:
    """
    æ‰©å±•çš„ BLIP2 æ¨¡å‹åŒ…è£…å™¨
    ä¸ºæ²¡æœ‰åœºæ™¯åˆ†ç±»å¤´çš„æ¨¡å‹æ·»åŠ é›¶æ ·æœ¬åˆ†ç±»èƒ½åŠ›
    """
    def __init__(self, base_model, use_zero_shot=False):
        """
        Args:
            base_model: åŸå§‹ BLIP2 æ¨¡å‹
            use_zero_shot: æ˜¯å¦ä½¿ç”¨é›¶æ ·æœ¬åˆ†ç±»
        """
        self.base_model = base_model
        self.use_zero_shot = use_zero_shot
        self.zero_shot_classifier = None
        
        if use_zero_shot:
            print(f"  ğŸ¯ åˆå§‹åŒ–é›¶æ ·æœ¬åˆ†ç±»å™¨...")
            try:
                self.zero_shot_classifier = ZeroShotSceneClassifier(
                    base_model,
                    base_model.device
                )
            except Exception as e:
                print(f"  âš ï¸ é›¶æ ·æœ¬åˆ†ç±»å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.zero_shot_classifier = None
    
    def extract_features(self, images, texts):
        """æå–ç‰¹å¾"""
        return self.base_model.extract_features(images, texts)
    
    def classify_scene(self, images):
        """åœºæ™¯åˆ†ç±»"""
        # ä¼˜å…ˆä½¿ç”¨åŸç”Ÿåˆ†ç±»å¤´
        try:
            native_logits = self.base_model.classify_scene(images)
            if native_logits is not None:
                return native_logits
        except Exception:
            pass
        
        # ä½¿ç”¨é›¶æ ·æœ¬åˆ†ç±»
        if self.use_zero_shot and self.zero_shot_classifier:
            return self.zero_shot_classifier.predict(images)
        
        return None
    
    @property
    def model_name(self):
        return getattr(self.base_model, 'model_name', 'BLIP2-LoRA')
    
    @property
    def device(self):
        return self.base_model.device


# ============================================
# æµ‹è¯•æ•°æ®é›†
# ============================================
class ClassificationTestDataset(torch.utils.data.Dataset):
    """åœºæ™¯åˆ†ç±»æµ‹è¯•æ•°æ®é›†"""
    
    def __init__(self, annotation_file: str, image_dir: str):
        print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {annotation_file}")
        
        with open(annotation_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        self.image_dir = image_dir
        
        # éªŒè¯æ•°æ®
        valid_data = []
        for item in self.data:
            if 'file_name' not in item or 'scene_category' not in item:
                continue
            
            scene = item['scene_category']
            if scene not in SCENE_TO_ID:
                continue
            
            image_path = os.path.join(self.image_dir, item['file_name'])
            if not os.path.exists(image_path):
                continue
            
            item['scene_id'] = SCENE_TO_ID[scene]
            valid_data.append(item)
        
        self.data = valid_data
        print(f"âœ… åŠ è½½ {len(self.data)} ä¸ªæœ‰æ•ˆæ ·æœ¬\n")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        image_path = os.path.join(self.image_dir, item['file_name'])
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å¤±è´¥: {image_path}, {e}")
            image = Image.new('RGB', (224, 224), color='red')
        
        return {
            'image': image,
            'scene_label': item['scene_id'],
            'file_name': item['file_name'],
        }


# ============================================
# è¯„ä¼°å•ä¸ªæ¨¡å‹
# ============================================
@torch.no_grad()
def evaluate_model(model, test_loader, device, exp_name):
    """
    è¯„ä¼°å•ä¸ªæ¨¡å‹çš„åœºæ™¯åˆ†ç±»æ€§èƒ½
    
    Args:
        model: æ¨¡å‹åŒ…è£…å™¨
        test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        exp_name: å®éªŒåç§°
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸ”¬ è¯„ä¼°å®éªŒ: {exp_name}")
    print(f"{'='*60}")
    
    all_labels = []
    all_preds = []
    all_probs = []
    
    # é€æ‰¹æ¬¡é¢„æµ‹
    for batch in tqdm(test_loader, desc="æ¨ç†ä¸­"):
        images = batch['image']
        labels = batch['scene_label']
        
        try:
            # åœºæ™¯åˆ†ç±»
            logits = model.classify_scene(images)
            
            if logits is None:
                raise ValueError(f"æ¨¡å‹ä¸æ”¯æŒåœºæ™¯åˆ†ç±»")
            
            # è®¡ç®—é¢„æµ‹
            probs = F.softmax(logits, dim=-1)
            preds = torch.argmax(logits, dim=-1)
            
            all_labels.append(labels)
            all_preds.append(preds.cpu())
            all_probs.append(probs.cpu())
        
        except Exception as e:
            print(f"âš ï¸ æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if len(all_labels) == 0:
        print(f"âŒ æ²¡æœ‰æˆåŠŸå¤„ç†çš„æ‰¹æ¬¡")
        return None
    
    # åˆå¹¶ç»“æœ
    all_labels = torch.cat(all_labels, dim=0).numpy()
    all_preds = torch.cat(all_preds, dim=0).numpy()
    all_probs = torch.cat(all_probs, dim=0).numpy()
    
    # ========== è®¡ç®—æŒ‡æ ‡ ==========
    
    # 1. æ€»ä½“å‡†ç¡®ç‡
    accuracy = accuracy_score(all_labels, all_preds) * 100
    
    # 2. æ¯ç±»åˆ«æŒ‡æ ‡
    precision, recall, f1, support = precision_recall_fscore_support(
        all_labels, all_preds, average=None, zero_division=0
    )
    
    # 3. åŠ æƒå¹³å‡
    precision_avg, recall_avg, f1_avg, _ = precision_recall_fscore_support(
        all_labels, all_preds, average='weighted', zero_division=0
    )
    
    # 4. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds)
    
    # 5. Top-3 å‡†ç¡®ç‡
    top3_preds = np.argsort(all_probs, axis=1)[:, -3:]
    top3_correct = np.array([label in top3_preds[i] for i, label in enumerate(all_labels)])
    top3_acc = top3_correct.mean() * 100
    
    # ========== æ‰“å°ç»“æœ ==========
    print(f"\nğŸ“Š åˆ†ç±»ç»“æœ:")
    print(f"  æ•´ä½“å‡†ç¡®ç‡: {accuracy:.2f}%")
    print(f"  Top-3 å‡†ç¡®ç‡: {top3_acc:.2f}%")
    print(f"  åŠ æƒ F1 åˆ†æ•°: {f1_avg*100:.2f}%")
    
    print(f"\nğŸ“‹ å„ç±»åˆ«å‡†ç¡®ç‡:")
    class_metrics = {}
    for i, scene_name in enumerate(SCENE_CATEGORIES):
        if support[i] > 0:
            class_acc = cm[i, i] / cm[i].sum() * 100 if cm[i].sum() > 0 else 0.0
            class_metrics[scene_name] = {
                'accuracy': float(class_acc),
                'precision': float(precision[i] * 100),
                'recall': float(recall[i] * 100),
                'f1': float(f1[i] * 100),
                'support': int(support[i]),
            }
            print(f"  {scene_name:8s}: {class_acc:6.2f}%")
    
    # ========== è¿”å›ç»“æœ ==========
    results = {
        'experiment': exp_name,
        'overall_accuracy': float(accuracy),
        'top3_accuracy': float(top3_acc),
        'weighted_f1': float(f1_avg * 100),
        'weighted_precision': float(precision_avg * 100),
        'weighted_recall': float(recall_avg * 100),
        'per_class': class_metrics,
        'confusion_matrix': cm.tolist(),
        'num_samples': len(all_labels),
    }
    
    return results


# ============================================
# å¯è§†åŒ–å‡½æ•°
# ============================================
def plot_comparison(all_results, output_dir):
    """ç»˜åˆ¶å¯¹æ¯”å›¾"""
    
    # æå–æ•°æ®
    exp_names = [r['experiment'] for r in all_results]
    overall_accs = [r['overall_accuracy'] for r in all_results]
    top3_accs = [r['top3_accuracy'] for r in all_results]
    f1_scores = [r['weighted_f1'] for r in all_results]
    
    # ä¸­æ–‡å®éªŒåæ˜ å°„
    name_map = {
        'blip2_base': 'BLIP2 Base(é›¶æ ·æœ¬)',
        'itc_only': 'ä»…ITC(é›¶æ ·æœ¬)',
        'scene_only': 'ä»…åœºæ™¯',
        'itc_scene_equal': 'ITC+åœºæ™¯(1:1)',
        'itc_scene_2_8': 'ITC+åœºæ™¯(2:8)',
        'itc_scene_8_2': 'ITC+åœºæ™¯(8:2)',
        'itc_itm': 'ITC+ITM(é›¶æ ·æœ¬)',
        'itc_scene_itm': 'ITC+åœºæ™¯+ITM',
    }
    display_names = [name_map.get(name, name) for name in exp_names]
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    # 1. æ•´ä½“å‡†ç¡®ç‡
    ax = axes[0, 0]
    bars = ax.barh(display_names, overall_accs, color='skyblue')
    ax.set_xlabel('æ•´ä½“å‡†ç¡®ç‡ (%)', fontsize=12)
    ax.set_title('åœºæ™¯åˆ†ç±»å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_xlim([0, 100])
    for bar, acc in zip(bars, overall_accs):
        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                f'{acc:.2f}%', va='center', fontsize=10)
    
    # 2. Top-3 å‡†ç¡®ç‡
    ax = axes[0, 1]
    bars = ax.barh(display_names, top3_accs, color='lightgreen')
    ax.set_xlabel('Top-3 å‡†ç¡®ç‡ (%)', fontsize=12)
    ax.set_title('Top-3 å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_xlim([0, 100])
    for bar, acc in zip(bars, top3_accs):
        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                f'{acc:.2f}%', va='center', fontsize=10)
    
    # 3. F1 åˆ†æ•°
    ax = axes[1, 0]
    bars = ax.barh(display_names, f1_scores, color='lightcoral')
    ax.set_xlabel('åŠ æƒ F1 åˆ†æ•° (%)', fontsize=12)
    ax.set_title('F1 åˆ†æ•°å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_xlim([0, 100])
    for bar, f1 in zip(bars, f1_scores):
        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                f'{f1:.2f}%', va='center', fontsize=10)
    
    # 4. å¹³å‡ç±»åˆ«å‡†ç¡®ç‡
    ax = axes[1, 1]
    avg_class_accs = []
    for result in all_results:
        class_accs = [m['accuracy'] for m in result['per_class'].values()]
        avg_class_accs.append(np.mean(class_accs) if class_accs else 0.0)
    
    bars = ax.barh(display_names, avg_class_accs, color='plum')
    ax.set_xlabel('å¹³å‡ç±»åˆ«å‡†ç¡®ç‡ (%)', fontsize=12)
    ax.set_title('å¹³å‡ç±»åˆ«å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_xlim([0, 100])
    for bar, acc in zip(bars, avg_class_accs):
        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                f'{acc:.2f}%', va='center', fontsize=10)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'ablation_scene_acc_comparison.png'), 
                dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š å¯¹æ¯”å›¾å·²ä¿å­˜")
    plt.close()


def plot_per_class_heatmap(all_results, output_dir):
    """ç»˜åˆ¶å„ç±»åˆ«å‡†ç¡®ç‡çƒ­åŠ›å›¾"""
    
    # æå–æ•°æ®
    exp_names = [r['experiment'] for r in all_results]
    
    # ä¸­æ–‡å®éªŒåæ˜ å°„
    name_map = {
        'blip2_base': 'BLIP2 Base(é›¶æ ·æœ¬)',
        'itc_only': 'ä»…ITC(é›¶æ ·æœ¬)',
        'scene_only': 'ä»…åœºæ™¯',
        'itc_scene_equal': 'ITC+åœºæ™¯(1:1)',
        'itc_scene_2_8': 'ITC+åœºæ™¯(2:8)',
        'itc_scene_8_2': 'ITC+åœºæ™¯(8:2)',
        'itc_itm': 'ITC+ITM(é›¶æ ·æœ¬)',
        'itc_scene_itm': 'ITC+åœºæ™¯+ITM',
    }
    display_names = [name_map.get(name, name) for name in exp_names]
    
    # æ„å»ºæ•°æ®çŸ©é˜µ
    data = []
    for result in all_results:
        row = []
        for scene in SCENE_CATEGORIES:
            if scene in result['per_class']:
                row.append(result['per_class'][scene]['accuracy'])
            else:
                row.append(0.0)
        data.append(row)
    
    data = np.array(data)
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    plt.figure(figsize=(14, max(6, len(exp_names) * 0.6)))
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']
    plt.rcParams['axes.unicode_minus'] = False
    
    sns.heatmap(
        data,
        annot=True,
        fmt='.1f',
        cmap='RdYlGn',
        xticklabels=SCENE_CATEGORIES,
        yticklabels=display_names,
        cbar_kws={'label': 'å‡†ç¡®ç‡ (%)'},
        vmin=0,
        vmax=100,
        linewidths=0.5,
    )
    
    plt.xlabel('åœºæ™¯ç±»åˆ«', fontsize=12)
    plt.ylabel('æ¶ˆèå®éªŒ', fontsize=12)
    plt.title('å„åœºæ™¯ç±»åˆ«å‡†ç¡®ç‡çƒ­åŠ›å›¾', fontsize=14, fontweight='bold', pad=20)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    plt.savefig(os.path.join(output_dir, 'ablation_per_class_heatmap.png'), 
                dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š çƒ­åŠ›å›¾å·²ä¿å­˜")
    plt.close()


def save_comparison_table(all_results, output_dir):
    """ä¿å­˜å¯¹æ¯”è¡¨æ ¼"""
    
    # æ€»ä½“æŒ‡æ ‡è¡¨
    rows = []
    for result in all_results:
        row = {
            'å®éªŒåç§°': result['experiment'],
            'æ•´ä½“å‡†ç¡®ç‡(%)': f"{result['overall_accuracy']:.2f}",
            'Top-3å‡†ç¡®ç‡(%)': f"{result['top3_accuracy']:.2f}",
            'F1åˆ†æ•°(%)': f"{result['weighted_f1']:.2f}",
            'ç²¾ç¡®ç‡(%)': f"{result['weighted_precision']:.2f}",
            'å¬å›ç‡(%)': f"{result['weighted_recall']:.2f}",
            'æ ·æœ¬æ•°': result['num_samples'],
        }
        rows.append(row)
    
    df_overall = pd.DataFrame(rows)
    df_overall = df_overall.sort_values('æ•´ä½“å‡†ç¡®ç‡(%)', ascending=False)
    
    overall_file = os.path.join(output_dir, 'ablation_overall_comparison.csv')
    df_overall.to_csv(overall_file, index=False, encoding='utf-8-sig')
    print(f"\nğŸ“„ æ€»ä½“å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜: {overall_file}")
    
    # æ‰“å°è¡¨æ ¼
    print(f"\n{'='*80}")
    print(f"ğŸ“Š æ¶ˆèå®éªŒåœºæ™¯åˆ†ç±»å‡†ç¡®ç‡å¯¹æ¯”")
    print(f"{'='*80}")
    print(df_overall.to_string(index=False))
    print(f"{'='*80}")
    
    # å„ç±»åˆ«è¯¦ç»†è¡¨
    class_rows = []
    for result in all_results:
        for scene, metrics in result['per_class'].items():
            class_rows.append({
                'å®éªŒåç§°': result['experiment'],
                'åœºæ™¯ç±»åˆ«': scene,
                'å‡†ç¡®ç‡(%)': f"{metrics['accuracy']:.2f}",
                'ç²¾ç¡®ç‡(%)': f"{metrics['precision']:.2f}",
                'å¬å›ç‡(%)': f"{metrics['recall']:.2f}",
                'F1åˆ†æ•°(%)': f"{metrics['f1']:.2f}",
                'æ ·æœ¬æ•°': metrics['support'],
            })
    
    df_class = pd.DataFrame(class_rows)
    class_file = os.path.join(output_dir, 'ablation_per_class_comparison.csv')
    df_class.to_csv(class_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ“„ ç±»åˆ«è¯¦ç»†å¯¹æ¯”è¡¨æ ¼å·²ä¿å­˜: {class_file}")
    
    return df_overall


# ============================================
# ä¸»å‡½æ•°
# ============================================
def main():
    """ä¸»å‡½æ•°"""
    
    config = {
        # æ•°æ®
        'test_file': '/workspace/vlm/lab/output/test_split.json',
        'image_dir': '/data/fasion/train/image',
        'batch_size': 32,
        'num_workers': 4,
        
        # è®¾å¤‡
        'device': 'cuda:4',
        
        # è¾“å‡º
        'output_dir': '/workspace/vlm/lab/output/ablation_scene_comparison',
        
        # åŸºç¡€ BLIP2 æƒé‡
        'base_checkpoint': 'checkpoint_04.pth',
    }
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['output_dir'], exist_ok=True)
    
    print("="*60)
    print("ğŸ”¬ æ¶ˆèå®éªŒåœºæ™¯åˆ†ç±»å‡†ç¡®ç‡å¯¹æ¯”")
    print("="*60)
    
    # ========== å®šä¹‰è¦å¯¹æ¯”çš„æ¶ˆèå®éªŒ ==========
    ablation_experiments = [
        # ğŸ”‘ æ·»åŠ  BLIP2 Base åŸºçº¿
        {
            'name': 'blip2_base',
            'display_name': 'BLIP2 Base(æœªå¾®è°ƒé›¶æ ·æœ¬)',
            'lora_path': None,  # ä¸åŠ è½½ LoRA
            'scene_head_path': None,
            'use_zero_shot': True,
            'is_base_model': True,  # æ ‡è®°ä¸ºåŸºç¡€æ¨¡å‹
        },
        {
            'name': 'itc_only',
            'display_name': 'ä»…ITCæŸå¤±(é›¶æ ·æœ¬)',
            'lora_path': 'outputs/ablation_itc_only/best_model',
            'scene_head_path': None,
            'use_zero_shot': True,
            'is_base_model': False,
        },
        {
            'name': 'scene_only',
            'display_name': 'ä»…åœºæ™¯åˆ†ç±»æŸå¤±',
            'lora_path': 'outputs/ablation_scene_only/best_model',
            'scene_head_path': 'outputs/ablation_scene_only/best_model/scene_head.pth',
            'use_zero_shot': False,
            'is_base_model': False,
        },
        {
            'name': 'itc_scene_equal',
            'display_name': 'ITC+åœºæ™¯(æƒé‡1:1)',
            'lora_path': 'outputs/ablation_itc_scene_equal/best_model',
            'scene_head_path': 'outputs/ablation_itc_scene_equal/best_model/scene_head.pth',
            'use_zero_shot': False,
            'is_base_model': False,
        },
        {
            'name': 'itc_scene_2_8',
            'display_name': 'ITC+åœºæ™¯(æƒé‡2:8)',
            'lora_path': 'outputs/ablation_itc_scene_2_8/best_model',
            'scene_head_path': 'outputs/ablation_itc_scene_2_8/best_model/scene_head.pth',
            'use_zero_shot': False,
            'is_base_model': False,
        },
        {
            'name': 'itc_scene_8_2',
            'display_name': 'ITC+åœºæ™¯(æƒé‡8:2)',
            'lora_path': 'outputs/ablation_itc_scene_8_2/best_model',
            'scene_head_path': 'outputs/ablation_itc_scene_8_2/best_model/scene_head.pth',
            'use_zero_shot': False,
            'is_base_model': False,
        },
        {
            'name': 'itc_itm',
            'display_name': 'ITC+ITM(é›¶æ ·æœ¬)',
            'lora_path': 'outputs/ablation_itc_itm/best_model',
            'scene_head_path': None,
            'use_zero_shot': True,
            'is_base_model': False,
        },
        {
            'name': 'itc_scene_itm',
            'display_name': 'ITC+åœºæ™¯+ITM(å®Œæ•´)',
            'lora_path': 'outputs/fashion_lora_itc_scene/best_model',
            'scene_head_path': 'outputs/fashion_lora_itc_scene/best_model/scene_head.pth',
            'use_zero_shot': False,
            'is_base_model': False,
        },
    ]
    
    # ========== åŠ è½½æµ‹è¯•æ•°æ® ==========
    print(f"\nğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®...")
    test_dataset = ClassificationTestDataset(
        config['test_file'],
        config['image_dir']
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=config['num_workers'],
        collate_fn=lambda x: {
            'image': [item['image'] for item in x],
            'scene_label': torch.tensor([item['scene_label'] for item in x]),
        }
    )
    
    # ========== è¯„ä¼°æ‰€æœ‰å®éªŒ ==========
    all_results = []
    
    for exp_config in ablation_experiments:
        try:
            print(f"\n{'#'*60}")
            print(f"# {exp_config['display_name']}")
            print(f"{'#'*60}")
            
            # ğŸ”‘ å¤„ç† BLIP2 Base æ¨¡å‹
            if exp_config.get('is_base_model', False):
                print(f"  ğŸ“¦ åŠ è½½ BLIP2 Base æœªå¾®è°ƒæ¨¡å‹...")
                
                # ç›´æ¥åŠ è½½åŸºç¡€ BLIP2 æ¨¡å‹ï¼ˆä¸åŠ è½½ LoRAï¼‰
                import sys
                sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
                from models.blip2_qformer import Blip2Qformer
                
                base_model = Blip2Qformer(
                    vit_model="clip_L",
                    img_size=224,
                    freeze_vit=True,
                    num_query_token=32,
                    embed_dim=256,
                    max_txt_len=77,
                )
                
                # åŠ è½½åŸºç¡€æƒé‡
                checkpoint = torch.load(config['base_checkpoint'], map_location='cpu')
                state_dict = checkpoint.get("model", checkpoint)
                base_model.load_state_dict(state_dict, strict=False)
                base_model.to(config['device'])
                base_model.eval()
                
                # è®¾ç½®å›¾åƒé¢„å¤„ç†
                from torchvision import transforms
                normalize = transforms.Normalize(
                    mean=[0.48145466, 0.4578275, 0.40821073],
                    std=[0.26862954, 0.26130258, 0.27577711]
                )
                base_model.transform = transforms.Compose([
                    transforms.Resize((224, 224)),
                    transforms.ToTensor(),
                    normalize,
                ])
                
                # æ·»åŠ å¿…è¦çš„æ–¹æ³•
                class BLIP2BaseWrapper:
                    def __init__(self, model, device):
                        self.model = model
                        self.device = device
                        self.model_name = "BLIP2-Base"
                    
                    @torch.no_grad()
                    def extract_features(self, images, texts):
                        if isinstance(images, list):
                            image_tensors = torch.stack([self.model.transform(img) for img in images])
                        else:
                            image_tensors = images
                        image_tensors = image_tensors.to(self.device)
                        
                        image_feats, text_feats = self.model({'image': image_tensors, 'text': texts})
                        
                        if image_feats.dim() == 3:
                            image_feats = image_feats.mean(dim=1)
                        
                        return image_feats, text_feats
                    
                    def classify_scene(self, images):
                        return None  # æ²¡æœ‰åˆ†ç±»å¤´
                
                base_model = BLIP2BaseWrapper(base_model, config['device'])
                print(f"  âœ… BLIP2 Base æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            else:
                # å¤„ç†å¾®è°ƒæ¨¡å‹
                if not os.path.exists(exp_config['lora_path']):
                    print(f"âš ï¸ LoRA æƒé‡ä¸å­˜åœ¨: {exp_config['lora_path']}")
                    print(f"è·³è¿‡...")
                    continue
                
                # åˆ›å»ºåŸºç¡€æ¨¡å‹
                base_model = create_model(
                    'blip2-lora',
                    device=config['device'],
                    base_checkpoint=config['base_checkpoint'],
                    lora_checkpoint=exp_config['lora_path'],
                    scene_head_path=exp_config['scene_head_path'],
                )
            
            # ğŸ”‘ å¦‚æœéœ€è¦é›¶æ ·æœ¬åˆ†ç±»ï¼ŒåŒ…è£…æ¨¡å‹
            if exp_config.get('use_zero_shot', False):
                print(f"  ğŸ¯ ä½¿ç”¨é›¶æ ·æœ¬åˆ†ç±»æ¨¡å¼")
                model = ExtendedBLIP2Wrapper(base_model, use_zero_shot=True)
            else:
                model = base_model
            
            # è¯„ä¼°
            results = evaluate_model(
                model,
                test_loader,
                config['device'],
                exp_config['name']
            )
            
            if results is not None:
                results['display_name'] = exp_config['display_name']
                results['classification_method'] = 'zero-shot' if exp_config.get('use_zero_shot') else 'supervised'
                results['is_base_model'] = exp_config.get('is_base_model', False)
                all_results.append(results)
                
                # ä¿å­˜å•ä¸ªå®éªŒç»“æœ
                result_file = os.path.join(config['output_dir'], 
                                          f"{exp_config['name']}_results.json")
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
            
            # é‡Šæ”¾å†…å­˜
            del model
            if 'base_model' in locals():
                del base_model
            torch.cuda.empty_cache()
        
        except Exception as e:
            print(f"âŒ è¯„ä¼° {exp_config['name']} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ========== ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š ==========
    if len(all_results) == 0:
        print("\nâŒ æ²¡æœ‰æˆåŠŸè¯„ä¼°çš„å®éªŒ")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
    print(f"{'='*60}")
    
    # 1. ä¿å­˜æ±‡æ€» JSON
    summary_file = os.path.join(config['output_dir'], 'ablation_summary.json')
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print(f"âœ… æ±‡æ€»ç»“æœå·²ä¿å­˜: {summary_file}")
    
    # 2. ä¿å­˜å¯¹æ¯”è¡¨æ ¼
    df_overall = save_comparison_table(all_results, config['output_dir'])
    
    # 3. ç»˜åˆ¶å¯¹æ¯”å›¾
    plot_comparison(all_results, config['output_dir'])
    
    # 4. ç»˜åˆ¶çƒ­åŠ›å›¾
    plot_per_class_heatmap(all_results, config['output_dir'])
    
    # ========== æœ€ç»ˆæ€»ç»“ ==========
    print(f"\n{'='*60}")
    print(f"âœ… æ¶ˆèå®éªŒå¯¹æ¯”å®Œæˆï¼")
    print(f"{'='*60}")
    
    print(f"\nğŸ† æ’å (æŒ‰åœºæ™¯åˆ†ç±»å‡†ç¡®ç‡):")
    sorted_results = sorted(all_results, key=lambda x: x['overall_accuracy'], reverse=True)
    for i, result in enumerate(sorted_results, 1):
        display_name = result.get('display_name', result['experiment'])
        method = result.get('classification_method', 'unknown')
        is_base = result.get('is_base_model', False)
        
        tags = []
        if is_base:
            tags.append('baseline')
        if method == 'zero-shot':
            tags.append('zero-shot')
        tag_str = f"[{', '.join(tags)}]" if tags else ""
        
        print(f"  {i}. {display_name:<35s} {tag_str:<20s}: {result['overall_accuracy']:.2f}%")
    
    print(f"\nğŸ’¡ æœ€ä½³é…ç½®: {sorted_results[0].get('display_name', sorted_results[0]['experiment'])}")
    print(f"   å‡†ç¡®ç‡: {sorted_results[0]['overall_accuracy']:.2f}%")
    print(f"   æ–¹æ³•: {sorted_results[0].get('classification_method', 'supervised')}")
    
    # å¯¹æ¯”åŸºçº¿çš„æå‡
    base_result = next((r for r in all_results if r.get('is_base_model')), None)
    best_result = sorted_results[0]
    if base_result and not best_result.get('is_base_model'):
        improvement = best_result['overall_accuracy'] - base_result['overall_accuracy']
        print(f"\nğŸ“ˆ ç›¸æ¯” BLIP2 Base æå‡: +{improvement:.2f}%")
    
    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {config['output_dir']}")
    print(f"\nğŸ“Š æŸ¥çœ‹å¯è§†åŒ–ç»“æœ:")
    print(f"   å¯¹æ¯”å›¾: {config['output_dir']}/ablation_scene_acc_comparison.png")
    print(f"   çƒ­åŠ›å›¾: {config['output_dir']}/ablation_per_class_heatmap.png")


if __name__ == "__main__":
    main()